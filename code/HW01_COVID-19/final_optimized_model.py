# æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬ - COVID-19é¢„æµ‹æ¨¡å‹
# åŸºäºtrain_comparison.pyè¿›è¡Œå…¨é¢ä¼˜åŒ–
# ä¿®å¤åŸä»£ç é”™è¯¯å¹¶å®ç°ç®€å•/ä¸­é˜¶/å¼ºåŸºçº¿

# PyTorch
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau
import torch.nn.functional as F

# For data preprocess
import numpy as np
import csv
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd

# For plotting
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

# é…ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

myseed = 42069  # set a random seed for reproducibility
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(myseed)
torch.manual_seed(myseed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(myseed)

def get_device():
    '''è‡ªåŠ¨é€‰æ‹©è®¡ç®—è®¾å¤‡'''
    return 'cuda' if torch.cuda.is_available() else 'cpu'

def plot_learning_curve(loss_record, title=''):
    '''ç»˜åˆ¶å­¦ä¹ æ›²çº¿'''
    total_steps = len(loss_record['train'])
    x_1 = range(total_steps)
    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]
    
    figure(figsize=(6, 4))
    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')
    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')
    plt.ylim(0.0, 5.)
    plt.xlabel('Training steps')
    plt.ylabel('MSE loss')
    plt.title('Learning curve of {}'.format(title))
    plt.legend()
    plt.show()

class COVID19Dataset(Dataset):
    '''ä¼˜åŒ–çš„COVID19æ•°æ®é›†ç±» - ä¿®å¤åŸå§‹é”™è¯¯'''
    def __init__(self, path, mode='train', target_only=False, feature_type='all', scaler=None):
        self.mode = mode
        self.scaler = scaler
        
        # è¯»å–æ•°æ®
        with open(path, 'r') as fp:
            data = list(csv.reader(fp))
            data = np.array(data[1:])[:, 1:].astype(float)
        
        # ç‰¹å¾é€‰æ‹© - ä¿®å¤åŸä»£ç ä¸­çš„passé”™è¯¯
        if feature_type == 'simple':
            # ç®€å•åŸºçº¿ï¼šä½¿ç”¨æ‰€æœ‰ç‰¹å¾
            feats = list(range(93))
        elif feature_type == 'medium':
            # ä¸­é˜¶åŸºçº¿ï¼š40ä¸ªå· + 2ä¸ªtested_positiveç‰¹å¾ (indices = 57 & 75)
            feats = list(range(40)) + [57, 75]  # ä¿®å¤åŸä»£ç é”™è¯¯
        elif feature_type == 'advanced':
            # å¼ºåŸºçº¿ï¼šç‰¹å¾å·¥ç¨‹ - é€‰æ‹©æ›´å¤šæœ‰ç”¨ç‰¹å¾
            # 40ä¸ªå· + ç—‡çŠ¶ç›¸å…³ç‰¹å¾ + æµ‹è¯•ç›¸å…³ç‰¹å¾ + æ—¶é—´è¶‹åŠ¿ç‰¹å¾
            feats = (list(range(40)) +           # 40ä¸ªå·ç‰¹å¾
                    [57, 75] +                   # tested_positiveç‰¹å¾
                    [41, 42, 43, 44, 45, 46] +   # ç—‡çŠ¶ç‰¹å¾(å’³å—½ã€å‘çƒ­ç­‰)
                    [47, 48, 49] +               # æ¥è§¦å²ç‰¹å¾
                    [50, 51, 52])                # å…¶ä»–ç›¸å…³ç‰¹å¾
        else:
            # é»˜è®¤ä½¿ç”¨æ‰€æœ‰ç‰¹å¾
            feats = list(range(93))
            
        if mode == 'test':
            # æµ‹è¯•æ•°æ®
            data = data[:, feats]
            self.data = torch.FloatTensor(data)
        else:
            # è®­ç»ƒæ•°æ®
            target = data[:, -1]
            data = data[:, feats]
            
            # ä¿®å¤æ•°æ®åˆ†å‰²é”™è¯¯ - ä½¿ç”¨sklearnçš„train_test_splitä»£æ›¿ç®€å•çš„æ¨¡è¿ç®—
            if mode == 'train':
                train_data, _, train_target, _ = train_test_split(
                    data, target, test_size=0.1, random_state=myseed, stratify=None)
                self.data = torch.FloatTensor(train_data)
                self.target = torch.FloatTensor(train_target)
            elif mode == 'dev':
                _, dev_data, _, dev_target = train_test_split(
                    data, target, test_size=0.1, random_state=myseed, stratify=None)
                self.data = torch.FloatTensor(dev_data)
                self.target = torch.FloatTensor(dev_target)
        
        # ä¿®å¤æ ‡å‡†åŒ–é”™è¯¯ - åªåœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—ç»Ÿè®¡é‡ï¼Œç„¶ååº”ç”¨åˆ°æ‰€æœ‰é›†åˆ
        if mode == 'train' and self.scaler is None:
            # åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆscaler
            self.scaler = StandardScaler()
            # åªæ ‡å‡†åŒ–éå·ç‰¹å¾ï¼ˆå·ç‰¹å¾å·²ç»æ˜¯one-hotç¼–ç ï¼‰
            if self.data.shape[1] > 40:
                self.data[:, 40:] = torch.FloatTensor(
                    self.scaler.fit_transform(self.data[:, 40:].numpy()))
        elif self.scaler is not None and self.data.shape[1] > 40:
            # åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡
            self.data[:, 40:] = torch.FloatTensor(
                self.scaler.transform(self.data[:, 40:].numpy()))
        
        self.dim = self.data.shape[1]
        print(f'Finished reading the {mode} set of COVID19 Dataset ({len(self.data)} samples found, each dim = {self.dim})')

    def __getitem__(self, index):
        if self.mode in ['train', 'dev']:
            return self.data[index], self.target[index]
        else:
            return self.data[index]

    def __len__(self):
        return len(self.data)

def prep_dataloader(path, mode, batch_size, feature_type='all', scaler=None):
    '''å‡†å¤‡æ•°æ®åŠ è½½å™¨'''
    dataset = COVID19Dataset(path, mode=mode, feature_type=feature_type, scaler=scaler)
    dataloader = DataLoader(
        dataset, batch_size,
        shuffle=(mode == 'train'), drop_last=False,
        num_workers=0, pin_memory=True)
    return dataloader, dataset.scaler if mode == 'train' else scaler

# ç®€å•åŸºçº¿æ¨¡å‹
class SimpleNet(nn.Module):
    '''ç®€å•åŸºçº¿ï¼šåŸºç¡€çš„ä¸¤å±‚ç½‘ç»œ'''
    def __init__(self, input_dim):
        super(SimpleNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        self.criterion = nn.MSELoss(reduction='mean')
    
    def forward(self, x):
        return self.net(x).squeeze(1)
    
    def cal_loss(self, pred, target):
        return self.criterion(pred, target)

# ä¸­é˜¶åŸºçº¿æ¨¡å‹
class MediumNet(nn.Module):
    '''ä¸­é˜¶åŸºçº¿ï¼šæ·»åŠ dropoutå’Œæ›´æ·±çš„ç½‘ç»œ'''
    def __init__(self, input_dim):
        super(MediumNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1)
        )
        self.criterion = nn.MSELoss(reduction='mean')
    
    def forward(self, x):
        return self.net(x).squeeze(1)
    
    def cal_loss(self, pred, target):
        return self.criterion(pred, target)

# å¼ºåŸºçº¿æ¨¡å‹
class AdvancedNet(nn.Module):
    '''å¼ºåŸºçº¿ï¼šæ·±åº¦ç½‘ç»œ + BatchNorm + Dropout + æ®‹å·®è¿æ¥ + L2æ­£åˆ™åŒ–'''
    def __init__(self, input_dim):
        super(AdvancedNet, self).__init__()
        
        # ä¸»å¹²ç½‘ç»œ
        self.layer1 = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        self.layer2 = nn.Sequential(
            nn.Linear(256, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.4)
        )
        
        self.layer3 = nn.Sequential(
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        self.layer4 = nn.Sequential(
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # æ®‹å·®è¿æ¥çš„æŠ•å½±å±‚
        self.shortcut = nn.Linear(256, 256)
        
        # è¾“å‡ºå±‚
        self.output = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1)
        )
        
        self.criterion = nn.MSELoss(reduction='mean')
    
    def forward(self, x):
        # ä¸»å¹²å‰å‘ä¼ æ’­
        x1 = self.layer1(x)           # input_dim -> 256
        x2 = self.layer2(x1)          # 256 -> 512
        x3 = self.layer3(x2)          # 512 -> 256
        
        # æ®‹å·®è¿æ¥ (x1 + x3)
        shortcut = self.shortcut(x1)  # ä¿è¯ç»´åº¦åŒ¹é…
        x3_residual = x3 + shortcut   # æ®‹å·®è¿æ¥
        
        x4 = self.layer4(x3_residual) # 256 -> 128
        output = self.output(x4)       # 128 -> 1
        
        return output.squeeze(1)
    
    def cal_loss(self, pred, target, l2_lambda=1e-4):
        '''è®¡ç®—æŸå¤± + L2æ­£åˆ™åŒ–'''
        mse_loss = self.criterion(pred, target)
        
        # L2æ­£åˆ™åŒ–
        l2_penalty = 0
        for param in self.parameters():
            l2_penalty += torch.norm(param, p=2)
        
        total_loss = mse_loss + l2_lambda * l2_penalty
        return total_loss

def train(tr_set, dv_set, model, config, device):
    '''è®­ç»ƒå‡½æ•° - æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦'''
    n_epochs = config['n_epochs']
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = getattr(torch.optim, config['optimizer'])(
        model.parameters(), **config['optim_hparas'])
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if config.get('scheduler'):
        if config['scheduler']['type'] == 'StepLR':
            scheduler = StepLR(optimizer, step_size=config['scheduler']['step_size'], 
                             gamma=config['scheduler']['gamma'])
        elif config['scheduler']['type'] == 'ReduceLROnPlateau':
            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50)
    else:
        scheduler = None
    
    min_mse = 1000.
    loss_record = {'train': [], 'dev': []}
    early_stop_cnt = 0
    epoch = 0
    
    while epoch < n_epochs:
        model.train()
        for x, y in tr_set:
            optimizer.zero_grad()
            x, y = x.to(device), y.to(device)
            pred = model(x)
            
            # æ ¹æ®æ¨¡å‹ç±»å‹è®¡ç®—æŸå¤±
            if isinstance(model, AdvancedNet):
                mse_loss = model.cal_loss(pred, y, l2_lambda=config.get('l2_lambda', 1e-4))
            else:
                mse_loss = model.cal_loss(pred, y)
                
            mse_loss.backward()
            optimizer.step()
            loss_record['train'].append(mse_loss.detach().cpu().item())
        
        # éªŒè¯
        dev_mse = dev(dv_set, model, device)
        if dev_mse < min_mse:
            min_mse = dev_mse
            print('Saving model (epoch = {:4d}, loss = {:.4f})'.format(epoch + 1, min_mse))
            torch.save(model.state_dict(), config['save_path'])
            early_stop_cnt = 0
        else:
            early_stop_cnt += 1
        
        epoch += 1
        loss_record['dev'].append(dev_mse)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler:
            if isinstance(scheduler, ReduceLROnPlateau):
                scheduler.step(dev_mse)
            else:
                scheduler.step()
        
        if early_stop_cnt > config['early_stop']:
            break
    
    print('Finished training after {} epochs'.format(epoch))
    return min_mse, loss_record

def dev(dv_set, model, device):
    '''éªŒè¯å‡½æ•°'''
    model.eval()
    total_loss = 0
    for x, y in dv_set:
        x, y = x.to(device), y.to(device)
        with torch.no_grad():
            pred = model(x)
            if isinstance(model, AdvancedNet):
                mse_loss = model.cal_loss(pred, y, l2_lambda=0)  # éªŒè¯æ—¶ä¸åŠ æ­£åˆ™åŒ–
            else:
                mse_loss = model.cal_loss(pred, y)
        total_loss += mse_loss.detach().cpu().item() * len(x)
    total_loss = total_loss / len(dv_set.dataset)
    return total_loss

def comprehensive_comparison():
    '''å…¨é¢çš„æ¨¡å‹å¯¹æ¯”å®éªŒ'''
    
    tr_path = 'covid.train.csv'
    tt_path = 'covid.test.csv'
    device = get_device()
    os.makedirs('models', exist_ok=True)
    
    print("="*80)
    print("COVID-19é¢„æµ‹æ¨¡å‹å…¨é¢å¯¹æ¯”å®éªŒ")
    print("ç®€å•åŸºçº¿ vs ä¸­é˜¶åŸºçº¿ vs å¼ºåŸºçº¿")
    print("="*80)
    
    results = {}
    
    # 1. ç®€å•åŸºçº¿
    print("\n1. è®­ç»ƒç®€å•åŸºçº¿æ¨¡å‹...")
    simple_config = {
        'n_epochs': 800,
        'batch_size': 270,
        'optimizer': 'SGD',
        'optim_hparas': {'lr': 0.001, 'momentum': 0.9},
        'early_stop': 150,
        'save_path': 'models/simple_baseline.pth'
    }
    
    tr_set, scaler = prep_dataloader(tr_path, 'train', simple_config['batch_size'], 'simple')
    dv_set, _ = prep_dataloader(tr_path, 'dev', simple_config['batch_size'], 'simple', scaler)
    
    simple_model = SimpleNet(tr_set.dataset.dim).to(device)
    simple_loss, simple_record = train(tr_set, dv_set, simple_model, simple_config, device)
    results['simple'] = {'loss': simple_loss, 'record': simple_record, 'features': tr_set.dataset.dim}
    
    # 2. ä¸­é˜¶åŸºçº¿  
    print("\n2. è®­ç»ƒä¸­é˜¶åŸºçº¿æ¨¡å‹ï¼ˆç‰¹å¾é€‰æ‹© + æ”¹è¿›æ¶æ„ï¼‰...")
    medium_config = {
        'n_epochs': 1000,
        'batch_size': 128,
        'optimizer': 'Adam',
        'optim_hparas': {'lr': 0.001, 'weight_decay': 1e-5},
        'early_stop': 200,
        'save_path': 'models/medium_baseline.pth'
    }
    
    tr_set_med, scaler_med = prep_dataloader(tr_path, 'train', medium_config['batch_size'], 'medium')
    dv_set_med, _ = prep_dataloader(tr_path, 'dev', medium_config['batch_size'], 'medium', scaler_med)
    
    medium_model = MediumNet(tr_set_med.dataset.dim).to(device)
    medium_loss, medium_record = train(tr_set_med, dv_set_med, medium_model, medium_config, device)
    results['medium'] = {'loss': medium_loss, 'record': medium_record, 'features': tr_set_med.dataset.dim}
    
    # 3. å¼ºåŸºçº¿
    print("\n3. è®­ç»ƒå¼ºåŸºçº¿æ¨¡å‹ï¼ˆæ·±åº¦ç½‘ç»œ + å…¨é¢ä¼˜åŒ–ï¼‰...")
    advanced_config = {
        'n_epochs': 1200,
        'batch_size': 64,
        'optimizer': 'Adam',
        'optim_hparas': {'lr': 0.001, 'weight_decay': 1e-4},
        'early_stop': 300,
        'l2_lambda': 1e-4,
        'scheduler': {
            'type': 'ReduceLROnPlateau'
        },
        'save_path': 'models/advanced_baseline.pth'
    }
    
    tr_set_adv, scaler_adv = prep_dataloader(tr_path, 'train', advanced_config['batch_size'], 'advanced')
    dv_set_adv, _ = prep_dataloader(tr_path, 'dev', advanced_config['batch_size'], 'advanced', scaler_adv)
    
    advanced_model = AdvancedNet(tr_set_adv.dataset.dim).to(device)
    advanced_loss, advanced_record = train(tr_set_adv, dv_set_adv, advanced_model, advanced_config, device)
    results['advanced'] = {'loss': advanced_loss, 'record': advanced_record, 'features': tr_set_adv.dataset.dim}
    
    # 4. ç»“æœå¯è§†åŒ–å’Œåˆ†æ
    print("\n4. ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    figure(figsize=(15, 5))
    
    # å­¦ä¹ æ›²çº¿å¯¹æ¯”
    plt.subplot(1, 3, 1)
    plt.plot(results['simple']['record']['dev'], label='ç®€å•åŸºçº¿', color='red', alpha=0.8)
    plt.plot(results['medium']['record']['dev'], label='ä¸­é˜¶åŸºçº¿', color='blue', alpha=0.8)
    plt.plot(results['advanced']['record']['dev'], label='å¼ºåŸºçº¿', color='green', alpha=0.8)
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('éªŒè¯æŸå¤±')
    plt.title('å­¦ä¹ æ›²çº¿å¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ€§èƒ½å¯¹æ¯”
    plt.subplot(1, 3, 2)
    models = ['ç®€å•åŸºçº¿', 'ä¸­é˜¶åŸºçº¿', 'å¼ºåŸºçº¿']
    losses = [results['simple']['loss'], results['medium']['loss'], results['advanced']['loss']]
    colors = ['red', 'blue', 'green']
    
    bars = plt.bar(models, losses, color=colors, alpha=0.7)
    plt.ylabel('éªŒè¯æŸå¤±')
    plt.title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
    plt.xticks(rotation=45)
    
    for bar, loss in zip(bars, losses):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{loss:.4f}', ha='center', va='bottom')
    
    # ç‰¹å¾æ•°é‡å¯¹æ¯”
    plt.subplot(1, 3, 3)
    feature_counts = [results['simple']['features'], results['medium']['features'], results['advanced']['features']]
    bars = plt.bar(models, feature_counts, color=colors, alpha=0.7)
    plt.ylabel('ç‰¹å¾æ•°é‡')
    plt.title('ä½¿ç”¨çš„ç‰¹å¾æ•°é‡')
    plt.xticks(rotation=45)
    
    for bar, count in zip(bars, feature_counts):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                f'{count}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('comprehensive_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # 5. è¯¦ç»†ç»“æœåˆ†æ
    print("\n" + "="*80)
    print("å®éªŒç»“æœè¯¦ç»†åˆ†æ:")
    print("="*80)
    
    for i, (name, result) in enumerate([('ç®€å•åŸºçº¿', results['simple']), 
                                       ('ä¸­é˜¶åŸºçº¿', results['medium']), 
                                       ('å¼ºåŸºçº¿', results['advanced'])]):
        print(f"\n{i+1}. {name}:")
        print(f"   éªŒè¯æŸå¤±: {result['loss']:.6f}")
        print(f"   ç‰¹å¾æ•°é‡: {result['features']}")
        print(f"   è®­ç»ƒè½®æ¬¡: {len(result['record']['dev'])}")
    
    # è®¡ç®—æ”¹è¿›å¹…åº¦
    simple_loss = results['simple']['loss']
    medium_loss = results['medium']['loss']
    advanced_loss = results['advanced']['loss']
    
    medium_improvement = ((simple_loss - medium_loss) / simple_loss) * 100
    advanced_improvement = ((simple_loss - advanced_loss) / simple_loss) * 100
    
    print(f"\næ€§èƒ½æ”¹è¿›åˆ†æ:")
    print(f"ä¸­é˜¶åŸºçº¿ç›¸å¯¹ç®€å•åŸºçº¿æ”¹è¿›: {medium_improvement:.2f}%")
    print(f"å¼ºåŸºçº¿ç›¸å¯¹ç®€å•åŸºçº¿æ”¹è¿›: {advanced_improvement:.2f}%")
    print(f"å¼ºåŸºçº¿ç›¸å¯¹ä¸­é˜¶åŸºçº¿æ”¹è¿›: {((medium_loss - advanced_loss) / medium_loss) * 100:.2f}%")
    
    # 6. ä¼˜åŒ–æ€»ç»“
    print(f"\n" + "="*80)
    print("ä¼˜åŒ–ç­–ç•¥æ€»ç»“:")
    print("="*80)
    print("1. ä¿®å¤çš„åŸä»£ç é”™è¯¯:")
    print("   âœ… ç‰¹å¾é€‰æ‹©æœªå®ç° (target_onlyçš„passè¯­å¥)")
    print("   âœ… æ ‡å‡†åŒ–ä½ç½®é”™è¯¯ (ä½¿ç”¨è®­ç»ƒé›†ç»Ÿè®¡é‡)")
    print("   âœ… æ•°æ®åˆ†å‰²æ–¹å¼ç®€å• (æ”¹ç”¨sklearn.train_test_split)")
    print("   âœ… ç¼ºå°‘L2æ­£åˆ™åŒ–å®ç°")
    print("   âœ… æ²¡æœ‰å­¦ä¹ ç‡è°ƒåº¦")
    
    print("\n2. å®ç°çš„ä¼˜åŒ–ç­–ç•¥:")
    print("   ğŸš€ ç‰¹å¾å·¥ç¨‹: 40å· + 2æµ‹è¯•é˜³æ€§ + ç—‡çŠ¶ç‰¹å¾")
    print("   ğŸš€ ç½‘ç»œæ¶æ„: æ®‹å·®è¿æ¥ + BatchNorm + Dropout")
    print("   ğŸš€ è®­ç»ƒç­–ç•¥: Adamä¼˜åŒ–å™¨ + å­¦ä¹ ç‡è°ƒåº¦ + L2æ­£åˆ™åŒ–")
    print("   ğŸš€ å®éªŒè®¾è®¡: ä¸‰çº§åŸºçº¿å¯¹æ¯” + å…¨é¢æ€§èƒ½åˆ†æ")
    
    return results

if __name__ == "__main__":
    comprehensive_comparison()

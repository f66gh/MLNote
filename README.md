# MLNote

## 基本概念

ML本质是一个将一个东西转化为另一个东西的方程

### 不同机器学习的类型

**回归 Regression**：这个方程输出一个数值

![image-20250902190507513](./pic/image-20250902190507513.png)

**分类 Classification**：这个方程输入固定的选项，输出正确值

![image-20250902190549537](./pic/image-20250902190549537.png)

例如阿尔法狗，从19*19选一个正确的选项

**Structured Learning**：生成一个有结构的东西，例如文章和代码

### 流程讲解

#### 1. 有未知参数的函数

模型是一个带有未知参数的方程

输入的已知参数叫feature，乘在feature前面的叫权重（weight），额外加的参数叫偏置（bias）

![image-20250902191507564](./pic/image-20250902191507564.png)

#### 2. 从训练数定义损失含函数

损失函数（LOSS）：是一个参数方程，输出值能够体现一组输入值（b,w）的好坏

![image-20250902192823275](./pic/image-20250902192823275.png)

每一次记录的数据都用写好的方程计算和真实数据的差值的绝对值（或者差值平方）称为error值，将所有error值进行计算便可以得到损失函数

#### 3. 最佳化（Optimization）

计算所有参数的损失函数后，需要找到Loss最小的参数
$$
w^*, b^* = arg\underset{w,b}{\min}L
$$
如何找到Loss最小，需要使用**梯度下降（Gradient Descent）**。就是计算偏导数那个东西

![image-20250902194020462](./pic/image-20250902194020462.png)

假设Loss只跟w一个参数有关系，随机选了一个$w_0$，计算这一点的导数，当导数为负时则要增加w，当导数为正时要减小w。

增加和减小w的值有两个东西决定：第一个是斜率大小，第二个是**η参数（学习速率，learning Rate）**。

学习速率你需要自己设定其大小。这种做机器学习中需要自己设定的东西叫**hyperparameters（超参数）**

你需要计算好下一个w的值：
$$
w_n = w_{n-1} - \eta \frac{\partial L}{\partial w} \Big|_{w = w_0}
$$
![image-20250902195526552](./pic/image-20250902195526552.png)

但是梯度下降只能找到**局部最小值**（极值）不能找到**全局最小值**，之后再探讨这个问题。

同理，有两个参数时，就需要计算偏导数（程序自动帮你算）。

![image-20250902195838416](./pic/image-20250902195838416.png)

![image-20250902200006598](./pic/image-20250902200006598.png)

若数据呈现周期性，则需要把方程适当调整。

![image-20250902200632040](./pic/image-20250902200632040.png)

 **model bias**：我们的线性模型总不会匹配真实的数据趋势，这个问题叫model bias

### sigmoid

![image-20250902213136767](./pic/image-20250902213136767.png)

事实上，一个波动的数据可以看做由多个三段折线构成

![image-20250902213518337](./pic/image-20250902213518337.png)

![image-20250902213631859](./pic/image-20250902213631859.png)

上面蓝色方程的要如何表达呢？用**Sigmoid**曲线逼近，叫做**Hard Sigmoid**
$$
y = c \frac{1}{1 + e^{-(b+wx_1)}} \\ = c \text{ sigmoid}(b+wx_1)
$$


![image-20250902214200664](./pic/image-20250902214200664.png)

![image-20250902214251208](./pic/image-20250902214251208.png)

![image-20250902214344659](./pic/image-20250902214344659.png)

可以将线性回归转化为有sigmoid的公式：

![image-20250902215518102](./pic/image-20250902215518102.png)

拓展，下面公式的含义
$$
y = b + \sum_{i} c_i \text{ sigmoid} \left( b_i + \sum_j w_{ij}x_j \right)
$$
这个公式是一个常见的两层神经网络结构

举个例子（感谢Gemini）：

我在买一个房子。我可能会考虑各个隐藏非客观存在因素，例如居住舒适度（i=1），升值潜力（i=2），交通便利性（i = 3）

1. x是输入参数，例如$x_1$卧室数量，$x_2$房屋面积，$x_3$地理位置等**原始特征**。
2. w是你给上述参数提供的**权重**，更看重哪一个需求。例如$w_{i1} = 3$，$w_{i2} = 10$, $w_{i3} = 2$（w值也会随着i变化而变化）

3. $b_i$为为每一个隐藏因素给的基本得分
4. sigmoid将后面一大坨转化为在开区间(0,1)内的值
5. c是给每一个隐藏因素都乘上一个**权重**，例如居住舒适度$c_1 = 10$，升值潜力$c_2 = 1$，交通便利性$c_3 = 8$。
6. b是最终的基础得分。

上面很接近人买一套房子的**多层次思考过程**。不直接用原始数据预测结果，而是用原始数据得到了更加抽象的因素，在根据这些因素获取到了最终结果。

这个公式描述的是一个常见的**两层神经网络结构**：

- **内层求和 ($∑_j$)：** 在括号内部，这个求和是对所有输入特征 xj 进行加权求和。它表示一个神经元（或者说一个`sigmoid`激活函数）的输入。这里的 wij 是连接第 j 个输入特征到第 i 个神经元的权重。
- **外层求和 ($∑_i$)：** 这个求和是对所有神经元的输出进行加权求和。每个 `sigmoid` 函数代表一个**隐藏层神经元**。ci 是每个隐藏层神经元的输出到最终输出的权重。

这个过程和人脑的工作方式非常相似，这也是为什么这种结构（被称为**神经网络**）在处理复杂问题时非常有效。公式描述的是一个**全连接网络**的一部分，其中每个“中间神经元”都考虑了**所有**输入信息。

用数学语言表达：

![image-20250902222819241](./pic/image-20250902222819241.png)

用向量和矩阵语言表达（向量**r**是sigmoid里边那一堆）：

![image-20250902223006461](./pic/image-20250902223006461.png)

每一个sigmoid函数都是上图的蓝色function，sigmoid函数输出也简写为向量**a**

![image-20250902223302944](./pic/image-20250902223302944.png)

权重c也可以表示为向量 **$c^T$**，最后再加上权重

![image-20250902223731699](./pic/image-20250902223731699.png)

用向量和常数表示双层神经网络的公式：
$$
y = b + \mathbf{c}^T \sigma(\mathbf{b} + \mathbf{W}\mathbf{x})
$$
![image-20250902223919412](./pic/image-20250902223919412.png) 

所有的未知参数拼起来的向量统称θ

![image-20250902224309542](./pic/image-20250902224309542.png)

有关于sigmoid方程计算loss：

![image-20250903110748148](./pic/image-20250903110748148.png)

最优化，要找到$θ^*$。需要选择一个初始向量$θ^0$，然后计算每一个未知参数对它的偏导数。所有偏导数组合到一起为梯度向量。之后再根据向量减法找出下一组$θ^1$，直至找到最优解。

**拓展：为什么梯度要做减法？因为你要找到loss的最小值，梯度的方向是函数增长最快的方向，所以需要反着找。**

梯度向量**g**是初始值（gradient）
$$
\mathbf{g} =  \begin{bmatrix} \left. \frac{\partial L}{\partial \theta_1} \right|_{\boldsymbol{\theta} = \boldsymbol{\theta}^0} \\ \left. \frac{\partial L}{\partial \theta_2} \right|_{\boldsymbol{\theta} = \boldsymbol{\theta}^0} \\ \vdots \end{bmatrix}
$$
简写为：
$$
\mathbf{g} = \nabla L(\boldsymbol{\theta}^0)
$$


做向量减法：
$$
\begin{bmatrix}
\theta_1^1 \\
\theta_2^1 \\
\vdots
\end{bmatrix}
=
\begin{bmatrix}
\theta_1^0 \\
\theta_2^0 \\
\vdots
\end{bmatrix}
-
\eta
\begin{bmatrix}
\left. \frac{\partial L}{\partial \theta_1} \right|_{\boldsymbol{\theta} = \boldsymbol{\theta}^0} \\
\left. \frac{\partial L}{\partial \theta_2} \right|_{\boldsymbol{\theta} = \boldsymbol{\theta}^0} \\
\vdots
\end{bmatrix}
$$
简写为：
$$
\boldsymbol{\theta}^1 = \boldsymbol{\theta}^0 - \eta \mathbf{g}
$$
在实际操作中，不会用所有参数只算一个Loss，随机分成了多个**batch**（一批数据）。每个batch单独计算Loss。

一个**Update**指的是一个batch的根据计算出的梯度调整未知参数的过程。

一个**epoch**（时期）指的是在模型训练中，完整地看了一遍所有的训练数据。

**所以一个epoch = 所有batch进行了一轮update**

在一个epoch过后要进行一次**Shuffle（置乱）**，再次随机生成一组batch。

![image-20250903112351738](./pic/image-20250903112351738.png)

sigmoid的平替：我不想用soft sigmoid把折线替代，也可以用另一种方法。

保持折线的方法：**ReLU(Rectified Linear Unit)**

![image-20250903113651422](./pic/image-20250903113651422.png)

 因为是两个折线合成的一个ReLU，所以需要用2i次求和。
$$
y = b + \sum_{2i}c_i \max(0, b_i + \sum_j w_{ij}x_j)
$$
**Sigmoid和ReLu统称为activation function （激活函数）**

### 为什么要分Batch

一整个数据集都放在一个batch里边叫**Full batch**

大batch每一个epoch只需要更新一次参数，但小batch每一个epoch需要更新多次参数。

![image-20250903191839411](./pic/image-20250903191839411.png)

注意不一定大batch更新速度就慢，小batch更新速度就慢。因为涉及到了**平行运算**，这是GPU干的活。

![image-20250903192250532](./pic/image-20250903192250532.png)

如下图所示，**一个大的batch反而可以节约时间。**

![image-20250903192407699](./pic/image-20250903192407699.png)

如下图所示，**用一个大的batch会导致loss变大**。这是**最优化**问题。

![image-20250903192902346](./pic/image-20250903192902346.png)

如下图所示，一种解释是小batch在一个epoch时会多次update参数，导致可能在一个batch卡在鞍点的参数在另一个batch中反而还能继续梯度下降。而大batch卡住就是卡住了。

![image-20250903193127286](./pic/image-20250903193127286.png)

在实际应用中，大Batch比小的Batch在测试集上差一点（训练集差不多）。即小batch比大batch**最优化**好一点。

![image-20250903193618615](./pic/image-20250903193618615.png)

这样的结果有一种解释：训练集得出的loss图形有一些局部最小值，这些局部最小值也分好坏。周围比较平的叫好局部最小值，周围比较波动的叫坏局部最小值。

当测试集和训练集的模式不太相同时，若参数对应的loss落在了好局部最小值则影响不大，若参数对应loss落在了坏局部最小值则会产生很大的偏差。

由于小batch会update很多次，参数及loss会随机变化，对于小batch的loss可能在sharp局部最小值很轻松的就跳出来，而大batch的loss只能比较严格的遵守梯度下降。

![image-20250903194613254](./pic/image-20250903194613254.png)

总结：

![image-20250903194951108](./pic/image-20250903194951108.png)

### 神经网络

同理，我们可以再多叠几层方程。

![image-20250903114235965](./pic/image-20250903114235965.png)

![image-20250903114608802](./pic/image-20250903114608802.png)

 神经网络并不是叠的越深越好，容易出现过拟合的现象。（多层网络反而不如浅层网络预测得好）

![image-20250903114755496](./pic/image-20250903114755496.png)

## 如何进行一个机器学习任务

![image-20250903115930449](./pic/image-20250903115930449.png)

### 当训练集loss很大时

#### 模型本身的问题（model bias）

可能是模型太简单，需要重新设定model。例如添加更多参数，以及转化为神经网络。

![image-20250903120209068](./pic/image-20250903120209068.png)

#### 最优化问题

梯度下降没办法找到全局最小值，只能找到局部最小值。

![image-20250903120951529](./pic/image-20250903120951529.png)

#### 如何判定当训练效果不佳是模型问题还是最优化问题？

 ![image-20250903121840228](./pic/image-20250903121840228.png)

测试集中，56层的神经网络不如20层的效果好，不叫过拟合。

训练集中，56层比20层效果还要差，这说明是模型的最优化没做好。

建议一开始选择简单的且最优化很好的模型训练。如果一个弹性更好的模型训练结果还不如简单的，那就是最优化的问题。

### 当训练集loss很小时

继续加深，越小越好。

若还是很大

#### 过拟合 Overfitting

**当训练集的Loss小，而测试集的loss大才叫过拟合。**为什么？

下图是一个极端的例子，训练集的Loss是0，测试集的loss很大

![image-20250903124216265](./pic/image-20250903124216265.png)

如果模型的自由度很大的话可能会导致过拟合。

![image-20250903124508138](./pic/image-20250903124508138.png)

如何解决过拟合？

* 增加训练集

* **数据增强（Data augmentation）**：例如在影响识别里边，把图片左右旋转放大缩小等，但也不是瞎操作，一般不能将图像上下颠倒。

![image-20250903124811988](./pic/image-20250903124811988.png)

#### mismatch

也可以算是overfitting，当训练资料和测试资料的分布不一样时，无论怎么增加训练集（增加训练集可以减少过拟合的问题），都没办法减小loss

![image-20250903130635154](./pic/image-20250903130635154.png)

### 如何选择一个在训练集和测试集Loss都很小的模型

当模型的弹性越大时，可能训练集的Loss会越来越小，但测试集的loss会变大。

找复杂的模型会有过拟合问题，找简单模型会有model bias问题。

![image-20250903125232355](./pic/image-20250903125232355.png)

也有可能从训练集选出的某一个很烂的模型在公共测试集上面恰好loss很低（这就是为什么训练集和测试集不能是一样的）

![image-20250903125537200](./pic/image-20250903125537200.png)

### 最优化失败的时候怎么办

上面讲的梯度下降方法中，当梯度为0时，loss就降不下去了

当梯度为0时，不一定是局部最小值（极值），也有可能是**saddle point(鞍点)**。**梯度为0的点统称为critical point**

例如$x^3$，当x为0时，导数值为0，但不是极值点，需要再求一次导确认。

![image-20250903165955467](./pic/image-20250903165955467.png)

loss卡在了局部最小值，则没有路可以走。但是如果卡在了鞍点，则还可以继续减小loss。

接下来需要判断loss被卡在哪一种情况。

#### 如何判断最优化失败时是由于局部最小值还是由于鞍点

复习线代：

1. 二次型是一个特殊的数学表达式，是一个多项式，其中所有项都是二次。
2. **海森矩阵（Hessian）**是一个由多元函数的二阶偏导数组成的矩阵。简单来说，海森矩阵**描述了损失函数在某个点附近的曲率**。它捕捉了函数在不同维度上的“弯曲”程度和方向。海森矩阵H的元素定义为

$$
H_{ij} = \frac{\partial^2 L}{\partial \theta_i \partial \theta_j}
$$

3. **正定（positive definite）**：当且仅当一个对称矩阵的所有**特征值都为正**时，它才是正定的。此时，对于所有非零向量 v，二次型 $v^THv>0$。

   **负定（negative definite）**：当且仅当一个对称矩阵的所有**特征值都为负**时，它才是负定的。此时，对于所有非零向量 v，二次型 $v^THv<0$。

   **不定**：当一个对称矩阵的特征值**有正有负**时，它是不定的。此时，二次型 $v^THv$的符号会随着向量 $v$的不同而改变。

复习泰勒展开：
$$
f(x) \approx f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \dots
$$
注意，**θ**以及**g**都是向量。
$$
\mathbf{g} = \nabla L(\boldsymbol{\theta'})
$$

$$
g_i = \frac{\partial L(\boldsymbol{\theta'})}{\partial \theta_i}
$$

下面的公式是泰勒展开的近似（少了二阶项之后的项），用矩阵和向量语言表达。
$$
L(\boldsymbol{\theta}) \approx L(\boldsymbol{\theta'}) + (\boldsymbol{\theta} - \boldsymbol{\theta'})^T \mathbf{g} + \frac{1}{2}(\boldsymbol{\theta} - \boldsymbol{\theta'})^T \mathbf{H}(\boldsymbol{\theta} - \boldsymbol{\theta'})
$$
![image-20250903171334740](./pic/image-20250903171334740.png)

当一阶项为0时，通过二阶项判断这一点附近的error surface
$$
L(\boldsymbol{\theta}) \approx L(\boldsymbol{\theta'}) + \frac{1}{2}(\boldsymbol{\theta} - \boldsymbol{\theta'})^T \mathbf{H}(\boldsymbol{\theta} - \boldsymbol{\theta'})
$$
![image-20250903171718107](./pic/image-20250903171718107.png)

下面的部分解释了如何通过**二次型**$v^THv$ 的符号来判断点的类型。这里的向量 $v $代表从 $θ^′$ 离开的任意方向，即 $v=(θ−θ^′)$。

- **局部最小值 (Local minima)**：$H$是正定的，故 $v^THv$总大于0。无论$θ$取何值二次项总大于0，故$L(θ)$总大于$L(θ^′)$。
- **局部最大值 (Local maxima)**：$H$是负定的，故 $v^THv$总小于0。无论$θ$取何值二次项总小于0，故$L(θ)$总小于$L(θ^′)$。
- **鞍点 (Saddle point)**：$v^THv$不固定正负，故$L(θ)$和$L(θ^′)$不一定谁大谁小。

在梯度为零的临界点，通过检查**海森矩阵的特性**，我们可以准确地判断这个点是**局部最小值**、**局部最大值**，还是**鞍点**。

![image-20250903172112081](./pic/image-20250903172112081.png)

 例子：
![image-20250903183550238](./pic/image-20250903183550238.png)

当$w_1$和$w_2$为0时，假设原始数据记为y-hat = 1，则会算出来这一点尽管两个偏导数为0（loss局部最小值），但其实是鞍点

![image-20250903183951769](./pic/image-20250903183951769.png)

海森矩阵也会告诉我们接下来应该往哪个方向找loss。

我们知道了要找到使得二次项为负的向量，让loss沿着这个向量update。上面知道了不定矩阵有负特征值，需要找到对应的**特征向量**。

拓展：为什么非要是负特征值的特征向量？

1. 特征向量代表了主曲率方向：海森矩阵的特征向量代表了损失函数在临界点处的**主曲率方向**（principal curvature directions）。这些方向是相互正交的，并且沿着这些方向，损失函数的曲率变化最为剧烈。

   * **负特征值**对应的特征向量指向损失函数**曲率最陡峭的下坡方向**。

   * **正特征值**对应的特征向量指向损失函数**曲率最陡峭的上坡方向**。

​	因此，选择负特征值对应的特征向量可以保证我们沿着**最有效的下坡路径**前进，从而更快地逃离鞍点。

2. 数学上的便利性：使用特征向量使得计算变得非常简单。

   - **通用向量**：对于任意向量 $v$，你需要计算矩阵乘法 $Hv$ 和向量内积 $v^T(Hv)$，这通常是一个复杂的计算。

   - **特征向量**：对于特征向量 $u$，计算简化为 $u^THu=u^T(λu)=λ∥u∥^2$。你只需要知道特征值 λ 和特征向量的范数 $∥u∥ $即可，这极大地简化了分析和计算。

3. 与牛顿法的联系：这种方法是**牛顿法**（Newton's Method）在非凸优化中的一种应用。标准的牛顿法通过海森矩阵的逆来更新参数：Δθ=−H−1g。在鞍点，梯度 g=0，牛顿法会停滞。

然而，当海森矩阵**不定**（有负特征值）时，我们可以利用特征分解 H=QΛQT，并修改更新规则，例如沿着负特征值对应的特征向量方向进行更新，这被称为**信赖域方法**（Trust-Region Method）等高级优化算法。这些方法正是基于对海森矩阵特征值的分析。

#### 局部最小值和鞍点哪一个更常见

这张幻灯片展示了一项**实证研究**，旨在探索在训练神经网络时，模型收敛到的**临界点**（critical point）是**鞍点**还是**局部最小值**。

图表是一个散点图，横轴是 **"Minimum Ratio"**（最小值比例），纵轴是 **"Training Loss"**（训练损失）。

- **横轴：最小值比例 (Minimum Ratio)**
  - 这个比例的定义是：`正特征值的数量 / 总特征值的数量`。
  - **如果这个比例接近 1**，意味着海森矩阵的大多数特征值都是正数，这表明该点更像一个**局部最小值**。
  - **如果这个比例远小于 1**，意味着海森矩阵有许多负特征值，这表明该点更像一个**鞍点**。
- **纵轴：训练损失 (Training Loss)**
  - 这表示模型在收敛到这个临界点时的损失函数值。

这张幻灯片的核心结论是：

1. 在神经网络训练中，优化器通常会收敛到**损失值较低**的临界点。
2. 这些损失值较低的临界点通常不是“纯粹”的局部最小值（即所有特征值都为正），而是一些**“很像”局部最小值**的点，因为它们的“最小值比例”非常接近 1。
3. 幻灯片上的文字指出：“**never reach a real local minima'**”，这暗示在实践中，模型可能永远不会找到一个完美的局部最小值（所有特征值都严格为正），而是找到一个**几乎所有方向都向上**的“平坦”区域。

这项实证研究支持了一个重要的观点：在**高维空间**中，**鞍点**比真正的局部最小值要常见得多。然而，优化算法（如梯度下降）在训练神经网络时，倾向于避开高损失值的鞍点，并收敛到损失值较低、更接近局部最小值的区域，这些区域可能有一些小的负曲率（即少量负特征值），但总体表现良好。

![image-20250903190438363](./pic/image-20250903190438363.png)



#### Momentum(动量)

若loss卡在了近似平坦的区域、鞍点以及局部最小值应该怎么办?

类似物理学中的球体沿着不规则斜面滚下去，也许可以冲出上面这些情况。

![image-20250903195311253](./pic/image-20250903195311253.png)

普通的梯度下降（Vanilla Gradient Descent）：沿着梯度的反方向调整参数

![image-20250903195532162](./pic/image-20250903195532162.png)

**Gradient Descent + Momentum**：每一步要变化的向量不只是这一点梯度的反方向向量，而是梯度反方向向量和上一步向量的和。
$$
\theta^n = \theta^{i-1} + m^{i},\space\space\space\space\space m^{i}=\lambda m^{i-1} -\eta g^{i-1}
$$
![image-20250903200509086](./pic/image-20250903200509086.png)

经过数学公式的转化，可以归纳成另一种方程：
$$
m^1 = \lambda m^0 - \eta g^0 = -\eta g^0 \\
m^2 = \lambda m^1 - \eta g^1 = -\lambda\eta g^0 - \eta g^1\\
m^3 = \lambda m^2 - \eta g^2 = -\lambda^2\eta g^0 - \lambda\eta g^1 - \eta g^2\\
\vdots \\
m^i = -\eta g^{i-1} - \lambda\eta g^{i-2} - \lambda^2\eta g^{i-3} - \dots - \lambda^{i-1}\eta g^0 \\
= -\eta \sum_{j=0}^{i-1} \lambda^{i-1-j} g^j
$$
这个方程的意思是本次的梯度的变化和前面所有梯度的变化都有关联。

![image-20250903201243896](./pic/image-20250903201243896.png)

如下图所示，即使在梯度为零的局部最小值点，也可以借助上一步的向量逃离被卡住的困境。

![image-20250903201707604](./pic/image-20250903201707604.png)

### 自适应学习率 adaptive Learning Rate

当loss不再下降时，真的有可能是遇到了局部最小值或者鞍点吗

![image-20250904092321038](./pic/image-20250904092321038.png)

 

学习率太大会导致参数值来回震荡，学习率太小则在loss相对平坦的地方停滞不前。

下面是一个凸（convex）出来的error surface

![image-20250904093117983](./pic/image-20250904093117983.png)

不同的参数需要不同的学习率。

原始梯度下降求参数公式：
$$
\theta_{i}^{t+1} = \theta_{i}^{t} - \eta g_{i}^{t}
$$

$$
g_{i}^{t} = \frac{\partial L}{\partial \theta_{i}} \bigg|_{\theta=\theta^{t}}
$$

加上自适应学习率的梯度下降公式：
$$
\theta_{i}^{t+1} = \theta_{i}^{t} - \frac{\eta}{\sigma_{i}^{t}} g_{i}^{t}
$$
**均方根（Root Mean Square）**计算自适应缩放因子$σ^t_i$
$$
\sigma_{i}^{t} = \sqrt{\frac{1}{t+1} \sum_{\tau=0}^{t} (g_{i}^{\tau})^2 }
$$
![image-20250904094958148](./pic/image-20250904094958148.png)

当loss很陡峭时，因子很大，改变参数的步伐很小。当Loss很平坦时，因子很小，改变参数的步伐很大。

![image-20250904095227926](./pic/image-20250904095227926.png)

当遇到一个复杂的error surface时，同一个参数同一个方向可能也需要动态调整学习率（？）

这就是**RMSProp**方法。
$$
\sigma_{i}^{t} = \sqrt{\alpha(\sigma_{i}^{t-1})^2 + (1-\alpha)(g_{i}^{t})^2}
$$


![image-20250904100006412](./pic/image-20250904100006412.png)

RMSProp 当前的梯度对参数变化有着很大的影响，而过往的梯度则影响较小。

![image-20250904101816165](./pic/image-20250904101816165.png)

现在最常用的最优化方法是**Adam**：RMSProp + Momentum

![image-20250904101934230](./pic/image-20250904101934230.png)

当使用RMS或者RMSProp时，由于平坦loss中梯度变化也要受到之前的陡峭loss中梯度变化很大的影响，故如下图所示会导致参数抖震荡得很厉害。

![image-20250904102352840](./pic/image-20250904102352840.png)

**学习率调度 Learning Rate Scheduling**：让学习率随着时间变化。可以衰减，也可以**warm up**.
$$
\theta_{i}^{t+1} = \theta_{i}^{t} - \frac{\eta^t}{\sigma_{i}^{t}} g_{i}^{t}
$$
![image-20250904103137453](./pic/image-20250904103137453.png)

综合公式：
$$
\theta_{i}^{t+1} = \theta_{i}^{t} - \frac{\eta^t}{\sigma_{i}^{t}} m_{i}^{t}
$$

* $η^t$：学习率调度（Learning Rate Scheduling），随着时间而变化的参数
* $σ^t_i$：梯度均方根（Root mean square of gradient），只考虑梯度的大小。
* $η^t / σ^t_i$：自适应学习率，**关注更新步长的大小，确保在不同坡度下都有合适的步长。**
* $m_i^t$：动量（Momentum）：是过往所有梯度的加权和，考虑梯度的方向，**平滑更新路径，减少局部震荡。**

![image-20250904103328516](./pic/image-20250904103328516.png)

### 分类 Classification

如何处理多分类任务？

![image-20250904105027702](./pic/image-20250904105027702.png)

把选项用一组**独热向量（One-Hot Vector）**表示，它的维度等于总类别数。对于任何一个类别，这个向量中只有一位是1，其余所有位都是0。

下图中的神经网络有三层：输入层、隐藏层和输出层。

1. **输入层 (x1,x2,x3)：** 您的数据输入。
2. **中间层 (r1,r2,r3)：** 接收输入层的信号，进行加权求和，然后传递给激活函数。
3. **隐藏层 (a1,a2,a3)：** 激活函数（图中的蓝色S形曲线）对中间层的输出进行非线性处理。
4. **输出层 (y1,y2,y3)：** 隐藏层的输出再经过一次加权求和，得到最终的输出向量。

最终的输出向量 y^ 就是神经网络对输入数据的预测。例如，如果模型的输出是 `[0.9, 0.05, 0.05]`，那么它最接近独热向量 `[1, 0, 0]`，所以模型预测该输入属于**类别1**。

![image-20250904105609057](./pic/image-20250904105609057.png)

在分类中，得出的结果需要用**softmax**函数转化一下，再和正确的独热向量进行对比。

Softmax 是一种用于**多分类问题的激活函数**。它的作用是把神经网络的原始输出（可以是任意实数）转换成一个概率分布。**这个概率分布中的每个值都在0到1之间，并且所有值的总和等于1。**（上图中的隐藏层的激活函数）

![image-20250904105732158](./pic/image-20250904105732158.png)

softmax函数计算过程分为两步：指数化和归一化，公式如下：
$$
y'_{i} = \frac{\exp(y_{i})}{\sum_{j}\exp(y_{j})}
$$
![image-20250904120327818](./pic/image-20250904120327818.png)

Mean Square Error（MSE）
$$
e = \sum_{i} (\hat{y}_{i} - y'_{i})^2
$$


**交叉熵 Cross-entropy** 
$$
e = - \sum_{i} \hat{y}_{i} \ln y'_{i}
$$
下一页PPT最后一行的意思是：**最小化交叉熵等价于最大化似然。**

复习概率论：

**似然（Likelihood）：** 这是衡量在给定模型参数下，观察到真实数据的概率有多大。

在数学上可以证明，对于分类问题，当模型预测的概率分布和真实标签的分布越接近时，交叉熵的值就越小。而同时，这也就意味着在当前模型参数下，**观察到这些真实标签的概率（即似然）越大**。

![image-20250904121112994](./pic/image-20250904121112994.png)

真实情况中，对于分类问题交叉熵用的最多。

如下图所示，MSE会导致loss很大，且loss大的地方很平滑，导致梯度下降会被卡在loss很大的地方。而交叉熵则相对更好移动一点。

甚至改变计算loss的方程都能改变最优化的困难度。

![image-20250904121729942](./pic/image-20250904121729942.png)



 ### 批量归一化 Batch Normalization

#### training

如下图所示，当两个输入的特征值改变时，权重也在变化。若多个特征值变化的范围比较接近，这种“碗状”的损失曲面对于梯度下降来说非常理想，因为梯度（红色箭头）总是指向中心，**训练可以平稳地收敛**。

若多个特征值变化的范围差异较大，在左侧的狭长曲面上，如果使用固定的学习率，梯度下降会遇到困难。梯度（红色箭头）的方向通常垂直于等高线。在狭长的峡谷中，梯度会沿着陡峭的 w2 方向剧烈震荡，但在平缓的 w1 方向上却进展缓慢。这导致训练过程不稳定且**收敛速度极慢**。**模型会在谷底两侧来回“弹跳”，却很难沿着谷底走向最优点。**

![image-20250904124456273](./pic/image-20250904124456273.png)

复习概率论：

在概率论和统计学中，Z-score 的公式是：
$$
Z=\frac{X - \mu}{\sigma}
$$
Z-score 的作用是将任何一个原始分数（X）转换为一个标准分数，这个分数表示该数据点偏离均值多少个标准差。例如，如果 Z-score 是 2，这意味着该数据点比平均值高出两个标准差。

这种处理方式在统计学上被称为**正态化（Normalization）**或**标准化（Standardization）**，它确保了数据在不同的维度上具有相同的尺度和分布特性，从而让梯度下降等优化算法能够更高效、更稳定地工作。

**批量归一化（Feature Normalization）**：特征标准化是一种数据预处理技术，它的目标是将每个特征的数值范围调整到一个统一的标准，**使得它们的均值为0，方差为1**（其实就是标准正态函数那个均值和方差）。

下图中的一个x是一组特征值组成的向量，对于每一行的第i个特征（$x^1_i,x^2_i,…,x_i^R$）需要进行标准化。对于每个维度i，首先计算其所有样本的**均值** ($m_i$) 和**标准差** ($σ_i$)。

然后用下面公式计算新的特征值：
$$
\tilde{x}_{i}^{r} = \frac{x_{i}^{r} - m_{i}}{\sigma_{i}}
$$
![image-20250904132048211](./pic/image-20250904132048211.png)

**标准化（Standardization）:**

在深度网络中，仅对输入特征进行正态化是不够的。随着数据经过多层线性计算，中间得到的特征向量（如 $z^1,z^2,z^3$）的数值分布可能会变得不稳定，**导致梯度消失或爆炸**。

* **数值差异：** 随着网络深度增加，激活值的数值范围可能会越来越大或越来越小，导致梯度在反向传播时变得非常大（梯度爆炸）或非常小（梯度消失）。

* **非线性饱和：** 即使使用像 Sigmoid 这样的激活函数，如果输入值过大或过小，函数曲线会进入“饱和”区域（即梯度接近于0）。这同样会导致梯度消失，使得网络无法有效学习。

因此，中间得到的向量值也需要**归一化**，需要和其他特征向量得出的中间值一起正态化，然后再经过激活函数（sigmoid）得到下一层需要的初始值。**确保了每一层送入激活函数的数值都处于一个稳定的、非饱和的区域.**

下图中是一个很大的网络，需要输入一个batch中的所有特征向量并计算。

 ![image-20250904140159332](./pic/image-20250904140159332.png)

**缩放和平移（scaling and Shifting）**：

* **目的：** 在标准化之后，批量归一化并不直接使用 $z^i$~。它引入了两个可学习的参数 γ（伽马）和 β（贝塔），对$z^i$~进行**缩放和平移**，得到最终的输出$z^i$。

* **公式：** $z^i$=γ⊙$z^i$~+β
  - 这里的 ⊙ 表示逐元素相乘。

**γ 和 β 的来源：** **它们是模型在训练过程中通过反向传播自动学习出来的参数**，就像权重 W 和偏置 b 一样。

在进行标准化得到z~时候，需要缩放和平移得到最终的z-hat。

如果只进行标准化，强制将所有激活值都限制在均值为0、方差为1的范围内，这可能会削弱网络层的**表达能力**。

例如，如果一个 Sigmoid 激活函数在它的最佳工作区域（非饱和区）是负数，但强制归一化到均值为0，可能会导致一些负值被挤压成正值。

引入 γ 和 β 的目的是：

- **恢复表达能力：** 它们允许网络**自适应地**学习一个最优的均值和标准差，而不必强制固定为0和1。
- **灵活性：**  当 γ 接近 σ 且 β 接近 μ 时，批量归一化层会接近于一个**恒等变换**（Identity Transformation），即不做任何改变，这给了网络“选择”不进行归一化的能力。
  - 当 γ 和 β 被学习为其他值时，网络可以根据需要将激活值调整到任何最优的分布，从而提高模型的学习效率和性能。

![image-20250904182124103](./pic/image-20250904182124103.png)

批量归一化的完整流程：

* 标准化
* 缩放与平移

#### testing

如果只给了一条测试集或者很少不足一个batch的测试集，那如何进行归一化？

在pytorch中，会自动帮你算好μ和σ。μ-bar 和 σ-bar是由每一个batch得出的μ和σ的加权平均值。

![image-20250904183405219](./pic/image-20250904183405219.png)



这张图的横轴是**训练步数（Steps）**，单位是百万（M），代表模型已经学习了多少次。纵轴是**准确率（Accuracy）**，代表模型的性能。

图中绘制了几条不同的曲线，它们代表了使用不同训练策略的模型：

- **黑色虚线（Inception）：** 这是不使用批量归一化的**基线模型**（Baseline Model）。它代表了原始 Inception 模型的训练过程。可以看到，它需要很长的训练时间（大约 20M 步）才能达到最高的准确率。
- **其他曲线（BN-Baseline, BN-x5, BN-x30, BN-x5~Sigmoid）：** 这些是使用了**批量归一化**的模型。
  - **红色点划线（BN-Baseline）：** 在基线模型上使用了批量归一化，但没有改变其他超参数。它的**收敛速度**明显快于黑色虚线，且最终的准确率也更高。
  - **蓝色实线（BN-x30）和蓝色虚点线（BN-x5）：** 这两组实验表明，在使用批量归一化后，模型可以采用**更大的学习率**（分别是基线学习率的 30 倍和 5 倍）。结果显示，即使使用更大的学习率，模型依然能够稳定收敛，并且收敛速度极快，最终性能也更高。
  - **紫色点划线（BN-x5~Sigmoid）：** 这条线展示了将 Sigmoid 激活函数用于批量归一化后的表现。虽然没有达到最优，但其性能也显著优于原始基线模型，说明批量归一化可以帮助激活函数避免饱和区域。

这张图清晰地表明，批量归一化带来了以下两大优势：

1. **显著提升收敛速度：** 批量归一化的模型（所有 BN 曲线）在更少的训练步数内就能达到更高的准确率。特别是当使用更大的学习率时，收敛速度的提升更为惊人。
2. **提高模型性能和稳定性：** 批量归一化不仅让训练更快，还使得模型能够达到更高的最终准确率。它还让模型对学习率等超参数不那么敏感，使得调参变得更容易。

![image-20250904183703997](./pic/image-20250904183703997.png)

## 卷积神经网络 CNN

![image-20250904210622778](./pic/image-20250904210622778.png)

![image-20250904210821116](./pic/image-20250904210821116.png)

在全连接网络中，从输入层到下一层的每个节点都有一个独立的权重。要计算权重总数，需要将输入特征值与输出单元数量相乘。这会导致需要计算的权重很多。

![image-20250904211318352](./pic/image-20250904211318352.png)

和人类识别物体类似，可以让神经网络只注意到物体的某些特征就可以判断其是否为这个物体。

![image-20250904211640976](./pic/image-20250904211640976.png)

### 感受野 Receptive field

这是一种简化全连接网络的一种方式。

每一个神经元只考虑自己的感受野

![image-20250904212122092](./pic/image-20250904212122092.png)

不同的神经元的感受野可以重合甚至一致。

![image-20250904212324148](./pic/image-20250904212324148.png)

最经典的感受野是会看图像所有的通道数（channel），高与宽合起来叫做**kernel size**（例如3*3）

**一般同一个感受野会有一组的神经元去守备。**

不同感受野之间会有一个**跨步 stride**（一般为1或者2），即两个感受野同一侧边界的距离。一般都会设置为有重叠的感受野，防止物体的某一个特征（pattern）给漏掉了。

若感受野有一部分超出了影像范围，则直接在**超出的部分填充（padding）**就行。一般可以填充0，复制图像边缘的参数等。

![image-20250904213833189](./pic/image-20250904213833189.png)

### 参数共享 Parameter Sharing 

如下图所示，鸟嘴可以在左上角也可能在中间出现。是否需要给每一个感受野对应的神经元都需要加上一个鸟嘴检测器？

![image-20250904214016287](./pic/image-20250904214016287.png)



在全连接网络中，每个神经元都有自己独立的权重。但在CNN中，同一个卷积核在处理输入数据的不同位置时，**会重复使用同一组权重（共享了参数）**。

拓展：为什么可以共享参数？

参数共享之所以可行，是基于一个核心的假设：**特征的局部性和平移不变性（locality and spatial invariance）**。

简单来说，这意味着**在图像（或任何数据）的一个区域中有效的特征，很可能在其他区域也同样有效**。

让我们用一个具体的例子来理解：

假设我们正在训练一个卷积神经网络来识别图像中的人脸。

1. **局部性**: 人脸的特征，例如眼睛、鼻子、嘴巴，都是在图像的**局部区域**内出现的。我们不需要看整张图像来识别一只眼睛，只需要关注眼睛周围的一小块区域。
2. **平移不变性**: 一个“眼睛”的特征，无论它出现在图像的左上角、右下角还是正中央，其本质都是相同的。如果一个卷积核（也就是一组权重）被训练来识别眼睛，那么这组权重应该能够识别**任何位置**的眼睛。

**所以，为什么我们可以共享参数？**

- **全连接网络**：如果你用全连接网络来处理图像，你需要为每个可能的眼睛位置都训练一个独立的、庞大的权重矩阵。比如，一个专门识别左上角眼睛的权重矩阵，一个专门识别右下角眼睛的权重矩阵，等等。这不仅参数量巨大，而且模型无法利用特征的平移不变性，因为它把每个位置的特征都当作了独立的新特征来学习。
- **卷积神经网络（参数共享）**：CNN通过一个小的卷积核来解决这个问题。这个卷积核就像一个“特征探测器”，它包含了一组权重。我们让这个卷积核在整个图像上滑动，在每个位置都进行计算。因为我们**共享了这同一组权重**，所以无论“眼睛”出现在图像的哪个位置，这个共享的卷积核都能有效地探测到它。
  - **优点**：
    1. **参数数量大大减少**：我们只需要学习一组权重（卷积核的权重），而不是为每个位置学习一组独立的权重。
    2. **提高了模型的泛化能力**：模型学会了识别一种“模式”（比如眼睛），而不是只在特定位置的“模式”。这使得模型对图像中物体的位置变化具有鲁棒性。

因此，参数共享是基于特征在不同位置具有相似性的这一事实，它极大地优化了神经网络的结构，使得模型更高效、更强大。

![image-20250904220150823](./pic/image-20250904220150823.png)

典型的共享参数

例如每一个感受野都有64个神经元在守备，每一个神经元的参数都不一样。但是**每个感受野都有一组拥有相同参数的神经元**，不同神经元但是对应相同的参数称为**filter 卷积核**。

每个卷积核都拥有一组独立的、可学习的**权重（parameters）**。

这张PPT用图示简洁地说明了CNN如何工作：

1. **一个卷积核**（比如filter 1）在整个输入数据上**滑动**，并使用**相同的权重**来计算每个位置的输出。这就是**参数共享**。
2. **多个卷积核**（filter 1, filter 2, filter 3...）同时对输入数据进行处理。每个卷积核都学习并提取**不同的特征**。
3. 最终，这些不同特征图被组合起来，形成一个更丰富、更有层次的表示，供后续的网络层使用。

![image-20250904220535028](./pic/image-20250904220535028.png)

结合使用感受野和参数共享的层叫**卷积层 Convolutional Layer**

全连接层弹性很高，适用于任何场景，但是容易造成过拟合，并不擅长任何一个特定领域。

卷积层弹性低，模型bias比较高，但是专门为影像设计。

![image-20250904221422173](./pic/image-20250904221422173.png)



### 两种简化方案的第二种解释

在一张6×6通道数为1的图像中，假设第一个卷积层有64个不同3×3大小的卷积核，则会生成4×4大小且通道数为64的图像。

![image-20250904222435808](./pic/image-20250904222435808.png)

![image-20250904222443847](./pic/image-20250904222443847.png)

第二个卷积层就需要若干个3×3大小通道数为64的卷积核

![image-20250904222719087](./pic/image-20250904222719087.png)

卷积核矩阵中的参数就是权重。

![image-20250904223106307](./pic/image-20250904223106307.png)

每一个卷积核在整张图片上滑动，这个过程叫**卷积（convolve）**。

### 池化 pooling

**下采样（Subsampling）**：对图像进行像素级的压缩或缩减并不会改变图像中物体的本质。

尽管右边的图片尺寸变小了，像素变少了，但我们仍然可以清楚地辨认出这是一只**鸟**。鸟的轮廓、颜色和主要特征并没有因为像素的减少而消失。

![image-20250904224502305](./pic/image-20250904224502305.png)

**最大池化 Max Pooling**：由卷积层得来的图像，每一个感受野仅留下一个最大的参数

![image-20250904224901570](./pic/image-20250904224901570.png)

一般情况下，卷积和池化是交替使用。池化主要用途是减少运算量，但随着近年来算力的提高，也不一定非要用池化。

![image-20250904225057414](./pic/image-20250904225057414.png)

在完整的CNN过程中，输出前还包括了flatten和全连接网络层以及softmax激活函数，最终得到了输出结果。

![image-20250904225145974](./pic/image-20250904225145974.png)

### CNN例子：阿尔法狗

下围棋是一个分类问题。棋盘作为一个图像仅有19×19的大小。每一个位置有48个通道，用于判断旁边是否有棋子、空白、是否要被吃掉等等

![image-20250904225407400](./pic/image-20250904225407400.png)

阿尔法狗没有用pooling。

![image-20250904225857343](./pic/image-20250904225857343.png)

CNN也可以用于语音和文字上。

![image-20250904225953756](./pic/image-20250904225953756.png)

## 自注意力机制 Self Attention

假设输入的参数是一组向量而不是一个，且向量的数量和长度会变化呢？（CNN是输入固定大小的图片）

![image-20250906155813724](./pic/image-20250906155813724.png)

例如：文字处理，可以把词汇表示为独热向量或者用 **Word Embedding 词嵌入** 表达

![image-20250906160125829](./pic/image-20250906160125829.png)

声音信息也可以表达为向量，通常窗口大小为25ms，一个窗口转化一个向量，每隔10ms会有一个窗口。

![image-20250906160359736](./pic/image-20250906160359736.png)

图、分子也可以表达为一个向量

![image-20250906160549939](./pic/image-20250906160549939.png)

![image-20250906160627927](./pic/image-20250906160627927.png)

多输入向量模型的输出也有不同，有的是输入多少向量输出就多少向量。例如文字处理的判断词性。

![image-20250906160917834](./pic/image-20250906160917834.png)

也有可能是一堆输入只对应一个输出，例如判断诊断是否积极还是消极。

![image-20250906161103548](./pic/image-20250906161103548.png)

也可能是需要机器自己决定要输出多少个。例如真正的语音输入。

![image-20250906161143829](./pic/image-20250906161143829.png)

### 序列标注 Sequence Labeling

这是输入和输出一样多的状况 

在判断单词词性时，只看一个单词不够。需要结合上下文，例如把前后两个相邻的向量一起输入。

但这种方法效果不好，若输入的窗口大到把整个训练集都塞到全连接网络中性能也会下降，且会导致过拟合的问题。这就引出了自适应注意力机制了。

![image-20250906161650037](./pic/image-20250906161650037.png)

自注意力会在全连接网络之前，考虑整个sequence才得到的一组新的向量。也可以把自注意力和全连接网络交替使用。下图中是展示**Transformer模型**架构的示意图。

这张图旨在帮助理解Transformer模型如何处理序列数据，比如一段文本中的词语。我们可以从下往上，分层来看这张图：

- **输入层（最底部）**：最底部的四个彩色矩形代表输入的序列，比如一句话中的四个词。每个矩形都是一个**词向量（Word Embedding）**，即将一个词转换为一个数字向量，以便计算机理解。
- **第一层自注意力（Self-attention）模块**：紧接着输入向量的是一个**自注意力模块**。这是Transformer模型的核心。这个模块的作用是让模型在处理序列中的每个词时，能够“注意”到序列中其他所有词的重要性。例如，当处理句子中的“它”这个词时，模型会通过自注意力机制，判断出“它”指的是前面提到的“猫”还是“狗”。
- **全连接层（FC）**：自注意力模块的输出，会经过一系列**全连接层**（FC，Fully Connected）。这些层对自注意力模块产生的输出进行进一步的非线性变换，提取更高层次的特征。
- **第二层自注意力模块**：在某些Transformer模型中，会堆叠多层自注意力模块和全连接层。第二层自注意力模块的作用与第一层类似，但它是在更高抽象层次上，对经过第一层处理的特征进行再次的“注意力”计算，这有助于模型捕捉更复杂、更长距离的依赖关系。
- **输出层（最顶部）**：顶部的四个彩色小圆圈代表最终的输出。在实际应用中，这些输出会根据具体的任务（如翻译、文本分类等）进行进一步处理。

![image-20250906162016082](./pic/image-20250906162016082.png)

b1是考虑了a1其他所有输入向量得来的。

![image-20250906162622973](./pic/image-20250906162622973.png)

自注意力机制会判断两个向量的关联程度，用α表示。最常用的是Dot-product方法，将输入的两个向量分别乘上两个矩阵，再把结果点乘得到α。

![image-20250906163449734](./pic/image-20250906163449734.png)

例如我们要知道a1和其他向量的**α 注意力分数（attention score）**，需要给a1乘上一个$W^q$矩阵，给其他向量乘上$W^k$矩阵（这里的q和w指的是query和key），然后再点乘得到$α_{1,n}$，经过softmax得到$α_{1,n}'$。

注意a1自己也要和自己进行上述操作。

![image-20250906163834341](./pic/image-20250906163834341.png)

为了知道a1和谁关联性最大，我们又计算了每一个向量的$v^i$，和自己的注意力分数相乘并累加。得到b1。

**a1和哪个向量关联性最强，b1就越接近哪个v。**

![image-20250906164402392](./pic/image-20250906164402392.png)

在自注意力中，b是同时生成的。如图所示。

![image-20250906165204047](./pic/image-20250906165204047.png)

注意softmax是对每一列的α归一化。

![image-20250906165442405](./pic/image-20250906165442405.png)

矩阵O就是自注意力输出的结果。

![image-20250906165620193](./pic/image-20250906165620193.png)

在自注意力中，只有$W^q,W^k,W^v$是**需要训练出来**的。

![image-20250906165800251](./pic/image-20250906165800251.png)

### 多头注意力机制 Multi-head Self-attention

若想得到多个不同种类的相关性，可以使用**多头注意力机制 Multi-head Self-attention**。

![image-20250906170246045](./pic/image-20250906170246045.png)

得到的b需要再乘上一个矩阵，多个相关性也输出一个矩阵。

![image-20250906170345099](./pic/image-20250906170345099.png)

### 位置编码 Positional Encoding

上面没有讲述向量（如词向量）的位置信息，可以加一个额外的**位置编码 Positional Encoding**。

为了弥补自注意力机制没有位置信息的缺陷，我们为每个词额外添加了一个“位置编码向量”，然后将它与词向量相加，形成一个包含词义和位置信息的混合向量，送入模型进行处理。

右侧的彩色图片是一个**位置编码矩阵**的可视化。

- **每一列**代表一个位置向量 ei，都代表一个**位置**。比如，第一列是位置 1 的编码向量，第二列是位置 2 的，以此类推。
- **每一行**代表位置向量中的一个维度。一个位置向量通常有很多个维度（比如 512 维）。
- **色彩**代表向量中数值的大小。你可以看到，这个矩阵的颜色呈现出一种独特的、像波浪一样的模式。这是因为位置编码公式使用了正弦和余弦函数，这种设计可以让模型更容易地学习到相对位置信息。例如，模型可以很容易地计算出两个词之间的距离，无论它们在序列中的哪个位置。

这种设计有一个神奇的特性：**任何两个位置，无论它们在序列中的哪个地方，它们之间的相对距离都可以被模型轻松地计算出来**。

为什么不用 1, 2, 3, 4…这样的整数来表示位置？ 主要有两个原因：

1. **无法扩展**：如果用整数，一个长句子（比如 100 个词）和另一个更长的句子（比如 500 个词）就会使用不同的编码，模型很难泛化到更长的句子。
2. **没有相对信息**：整数编码只能告诉你“这是第5个词”，但它无法告诉模型“第5个词和第10个词的距离是5”，而正余弦函数可以轻松地做到这一点。

![image-20250906172213702](./pic/image-20250906172213702.png)

### 自注意力机制的其他用途 & Self-attention v.s. CNN

自注意力机制除了用于transformer和NLP之外，也可以用于语音识别。

以语音识别为例，不可能把很长的语音都进行一次自注意力，消耗太大了。一般都是根据对资料的理解分割为多个段，这样计算量会小很多。

![image-20250906172819512](./pic/image-20250906172819512.png)

自注意力机制也可以用于图像识别，将一个像素的三个通道看成一个向量。

![image-20250906172941723](./pic/image-20250906172941723.png)

CNN只需要考虑一个感受野，而自注意力则需要考虑全局。从这个意义上来说，CNN是一个简化版的自注意力机制。自注意力机制中，感受野不再是人为划分，而是机器自己学出来的。

![image-20250906173155706](./pic/image-20250906173155706.png)

在训练集少的时候，CNN会更胜一筹，而训练集多的时候，自注意力会更好。

可以解释为自注意力机制弹性更好，但没有充足的训练则不知道怎么找合适的感受野。CNN人为设定了感受野，一开始就知道了空间局部性很重要。

![image-20250906173407576](./pic/image-20250906173407576.png)

### Self-attention v.s. RNN

**循环神经网络（Recurrent Neural Network）** 是一种专门设计来处理序列数据的模型，比如文本、语音或时间序列。

你可以将它想象成一个**串行处理的流水线**。如图上半部分所示：

- **逐个处理**：RNN 每次只处理序列中的**一个元素**（例如一个词）。
- **记忆**：它有一个“**记忆（memory）**”状态，用于存储之前处理过的信息。在处理当前词时，它会结合这个记忆状态和当前词的输入，然后更新记忆状态，并传递给下一个 RNN 单元。
- **串行**：这种工作方式是**非并行（nonparallel）**的，因为每个步骤都依赖于前一个步骤的输出。

| 特性 (Feature)                          | 循环神经网络 (RNN)                                | 自注意力 (Self-attention)                                    |
| :-------------------------------------- | :------------------------------------------------ | :----------------------------------------------------------- |
| **处理方式** (Processing)               | 串行 (Sequential): <br> 逐个处理序列中的元素。    | 并行 (Parallel): <br> 可以同时处理序列中的所有元素。         |
| **长距离依赖** (Long-range Dependency)  | 难以学习 (Hard to learn): <br> 记忆状态容易衰减。 | 易于学习 (Easy to learn): <br> 可以直接计算任意元素间的关系。 |
| **计算效率** (Computational Efficiency) | 低 (Low): <br> 串行处理限制了并行计算。           | 高 (High): <br> 可利用 GPU 并行计算。                        |

自注意力机制的胜利主要源于它的**并行处理能力**和**有效捕捉长距离依赖**的特性。这让它能够：

1. **更高效地训练**：并行计算让模型能够利用强大的硬件，大幅缩短训练时间。
2. **更强大的性能**：它能更好地理解整个序列的上下文，而不是仅仅依赖于前面的几个词。这在处理长文本时尤其重要，比如在做翻译、摘要或问答任务时。

正因如此，基于自注意力机制的 **Transformer** 模型在许多领域（特别是自然语言处理）已经取代了传统的 RNN，成为了主流架构。

![image-20250906174319556](./pic/image-20250906174319556.png)

### 自注意力用于图

只有相连的节点才需要计算注意力分数。这种把自注意力机制用于图的操作是一种**图神经网络Graph Neural Network（GNN）**

![image-20250906175103686](./pic/image-20250906175103686.png)



## Transformer

### Seq2seq

transformer的作用是输入一段序列，输出一段序列（**Seq2Seq**）。输出序列的长度取决于model。

输入语音，输出文字是语音辨识；输入文字，输出语音是语音合成。

![image-20250906185151391](./pic/image-20250906185151391.png)

输入问题和文字，输出答案也是seq2seq

![image-20250906190107075](./pic/image-20250906190107075.png)

Seq2Seq甚至可以做句法分析。

Seq2seq 模型将复杂的**句法分析任务**，转化为了一个相对简单的**序列转换任务**。它绕过了传统句法分析中需要手动设计规则的复杂过程，让模型直接从大量数据中学习如何将一个句子映射到其对应的**语法树**结构。

![image-20250906190452229](./pic/image-20250906190452229.png)

Seq2seq也可以做Multi-label 分类，例如一个文件不止一个分类标签，传统分类方法只能输出一个标签，也不能硬性指定输出分数最高的三个标签。用Seq2seq则会自己决定输出几个标签。

![image-20250906190944295](./pic/image-20250906190944295.png)

也可以用于物体识别。

![image-20250906191142523](./pic/image-20250906191142523.png)



**Seq2seq** 是一种通用的神经网络架构，用于将一个序列转换为另一个序列。它通常由一个**编码器 (Encoder)** 和一个**解码器 (Decoder)** 组成。

- **编码器**：读取输入的序列，并将其转换为一个“上下文向量”，捕捉整个序列的含义。
- **解码器**：根据编码器生成的上下文向量，逐个生成输出序列。



### Encoder

输入一组向量，输出相同长度的向量。

![image-20250906191339060](./pic/image-20250906191339060.png)

一个encoder中会有若干**block**

![image-20250906191813333](./pic/image-20250906191813333.png)

下面是transformer的编码器中一个block的结构。

1. **核心的自注意力（Self-attention）模块**

- 最底部是 **自注意力（Self-attention）** 层。正如我们之前讨论的，这一层的作用是让模型能够计算序列中每个元素与其他所有元素之间的关系，从而捕捉长距离依赖。

2. **残差连接（Residual Connection）**

- 自注意力层的输出（图中用 `a` 表示）并没有直接进入下一层。它被加到了自注意力层的**输入**（图中用 `b` 表示）上。这种操作被称为**残差连接（Residual Connection）**，图中用 `a + b` 表示。
- 残差连接的作用是：让信息能够“跳过”某些层，直接流向网络的更深层。这解决了深度神经网络在训练时遇到的一个主要问题——**梯度消失**。通过允许梯度直接回流，残差连接确保了即使在很深的层中，模型也能有效地学习。

3.  **层归一化（Layer Normalization）**

- 在残差连接之后，结果会经过一个 **层归一化（Layer Normalization）** 操作。图中用 `norm` 表示。
- 中间的白色框详细解释了层归一化：它对输入向量的**每个维度**进行归一化，使其具有均值 m 和标准差 σ。这个操作可以稳定每层输入的分布，从而加速训练过程并提高模型的稳定性。

4. **前馈神经网络（Feed-forward Network）**

- 经过自注意力层和归一化后，数据流向了一个**全连接层（FC）**，也就是**前馈神经网络**
- 注意前馈神经网络每一层都是全连接网络，图中描述的不太准确。
- 这个网络通常由两个全连接层组成，中间有一个激活函数。它的作用是对自注意力层的输出进行更复杂的非线性变换，以提取更高层次的特征。

5. **另一个残差连接和层归一化（下一个block了）**

- 在前馈神经网络的输出之后，同样有一个**残差连接**和**层归一化**。这表明整个 Transformer 编码器模块的结构是**对称**且**重复**的。

![image-20250906191833777](./pic/image-20250906191833777.png)

### Decoder

**自回归（Autoregressive）模型**：每次只预测序列中的下一个元素，并把这个预测结果作为下一次预测的输入。**解码器是一个自回归模型。**

如图所示，编码器输出的向量送往解码器，使用特殊的标记BEGIN启动预测过程。输出的结果用softmax归一化，选择概率最大的文字。

![image-20250906193642210](./pic/image-20250906193642210.png)

下图中展示了自回归模型如何一步步的生成完整地序列直至结束。

![image-20250906193806299](./pic/image-20250906193806299.png)

解码器和编码器结构比较类似，但是把编码器中的多头注意力机制换成了**掩码多头注意力（Masked Multi-Head Attention）**

![image-20250906194157957](./pic/image-20250906194157957.png)

如下图所示，掩码多头注意力生成的b2只会由a2和a1产生，而a3和a4对其没有影响。

![image-20250906194531871](./pic/image-20250906194531871.png)

自回归模型的一个核心特点是**串行生成**，也就是一个一个地生成输出序列。在预测第 i 个字时，模型只能使用**它之前已经生成的**所有字（从开始标记到第 i−1 个字），而**不能看到**或“作弊”式地使用第 i 个字之后的任何信息。

如果解码器使用了标准的无掩码多头注意力，它在预测第 i 个字时，会无意中“看到”整个序列的所有信息，包括未来的字。这就像是考试作弊，虽然模型可能学得很好，**但这种学习方式在实际应用（比如文本生成）中是无效的。**

![image-20250906194711838](./pic/image-20250906194711838.png)

需要一个END符号，作为整个序列输出的结束，否则序列就会一直持续下去。

![image-20250906195051510](./pic/image-20250906195051510.png)

![image-20250906195103040](./pic/image-20250906195103040.png)

### Decoder: Non-autoregressive（NAT）

NAT相较于AT一个一个串行生成，是输入一堆begin然后输出结果。

那我们是如何知道要生成序列的长度呢？

* 可以在使用NAT前先用一个预测器输出将要生成序列的长度
* 可以输出一个很长的序列，截断END后面的序列。

NAT的优点：快速，并且序列长度可控

NAT缺点：不如AT准。

拓展：**Multi-modality 多模态问题**

“多模态”在这里指的是**一个输入句子可以有多种不同的正确翻译**。例如： 

输入：`The car is fast.`

- 翻译 1：`这辆车很快。`
- 翻译 2：`这辆汽车速度很快。`

对于 AT 模型来说，这不是问题。它可以一步一步地，有条不紊地生成其中一个正确的翻译。

但对于 NAT 模型，这就成了一个大挑战。由于它**同时预测所有词**，它必须为每个位置选择一个词，而这些选择可能来自**不同的“正确翻译”**。

例如，在预测第一个词时，它可能选择“这辆车”。但在预测第四个词时，它可能选择了来自“这辆汽车速度很快”的“很快”。这就会导致最终生成的句子不连贯。

**这种混杂了不同正确翻译的可能性，就是 NAT 性能不如 AT 的主要原因。**

尽管 NAT 速度快，但它**为了并行而牺牲了词与词之间的依赖关系**。这使得它在处理具有多种可能翻译的复杂任务时，生成的句子**连贯性和质量**通常不如 AT。

![image-20250906200232697](./pic/image-20250906200232697.png)

### Encoder与Decoder之间是如何传递信息的

![image-20250906200356827](./pic/image-20250906200356827.png)

**跨注意力（Cross-attention）**机制的工作原理。

如图所示，BEGIN会先进入到掩码自注意力生成一个q，然后和encoder生成的k做点乘计算注意力分数α，结果再和v做加权求和，经过全连接网络输出一个结果。

![image-20250906200614444](./pic/image-20250906200614444.png)

同理，下一个输入也要重复这个流程。

**为什么不能只用一个编码器？一个编码器已经能够获得当前文字最有可能输出的结果了啊？**

你可以把编码器和解码器想象成一个翻译团队：

- **编码器是“理解员”**：它的工作是阅读并理解原文的全部内容。它把原文的每一个词、每一个句子背后的意思都压缩成一个高度浓缩的笔记。这个笔记包含了所有的信息，但它不是原文的直接翻译。
- **解码器是“翻译员”**：它的工作是根据“理解员”的笔记，一步一步地用目标语言把意思表达出来。它需要解决**语言的生成问题**，这包括：
  - **生成顺序**：哪个词先说，哪个词后说？
  - **语法和词汇**：如何选择正确的词汇和语法结构来表达意思？
  - **结束时机**：什么时候翻译结束？

所以，**编码器负责理解，解码器负责生成**。它们分工协作，缺一不可。编码器给了解码器一个好的“笔记”，而解码器则根据这个“笔记”，把信息以一种符合目标语言语法和逻辑的方式“写”出来。

如果把这两个功能混在一起，一个模型既要理解原文，又要生成译文，那么这个模型会变得极其复杂且难以训练，而且效果通常不会很好。这就是为什么 Transformer 架构选择将**理解和生成**这两个任务分给了编码器和解码器，各自专精其职，从而实现了强大的性能。

![image-20250906200931874](./pic/image-20250906200931874.png)

### Training 训练

每一个文字都是一个独热向量，解码器出来的结果和这些独热向量的交叉熵越小越好。

注意END也是一个独热向量，也是一个独特的信号。

在训练模型的每一步，都将**现实中正确的上一个输出（而不是模型自己预测的上一个输出）**作为模型当前步的输入。这种训练模式叫**Teacher Forcing（强制教学）**。

![image-20250906202709723](./pic/image-20250906202709723.png)

#### 复制机制 Copy Mechanism

例如聊天机器人，复述上一句话中的某些短语。

或者做一个文章的摘要。

![image-20250906203121688](./pic/image-20250906203121688.png)

#### Guided Attention

Guided Attention 是一种注意力机制的变体，它的目标是**引导或约束模型在注意力计算过程中，将注意力集中在特定的、预设的输入部分上**。

与标准的自注意力机制不同，自注意力是完全自由的，模型可以根据数据自己决定如何分配注意力。而 Guided Attention 则像是给注意力机制加了一根“缰绳”，强制它在某个时间步，只能关注输入序列的特定区域。

例如当机器阅读文字时，注意力应该是由前至后的，而不是跳跃式的。

![image-20250906203818287](./pic/image-20250906203818287.png)

#### 束搜索 Beam Search

左下角的红色路径代表**贪婪解码（Greedy Decoding）**。它的策略非常简单：**在每一步都选择当前概率最高的那个词**。

- **路径**：从起始点开始，模型在每一步都选择概率最高的路径。
- **示例**：在第一个分岔路口，如果 A 的概率是 0.4，B 的概率是 0.6，贪婪解码会毫不犹豫地选择 B。在下一个分岔路口，它会继续选择概率最高的那个。
- **缺点**：这种方法只考虑了当前这一步的最优选择，而**不考虑全局最优**。就像图中展示的，虽然红色路径每一步都看起来不错（0.6, 0.6），但最终却不是最好的整体路径。这说明局部最优不等于全局最优。

**束搜索（Beam Search）**是一种更高级、更智能的解码策略，它试图在**计算效率**和**全局最优**之间找到一个平衡。

- **基本思想**：它在每一步不是只保留一个最优路径，而是保留**多条**看起来最有希望的候选路径。这个“多条”的数量就是**束宽（beam width）**，通常是一个预设的超参数（比如 3、5、10）。
- **过程**：
  1. **第一步**：模型计算出所有可能的下一个词的概率，并选择概率最高的 **K** 个词作为候选，形成 **K** 条路径。
  2. **每一步**：对于每条现有的候选路径，模型都会计算它下一个词的所有可能概率，并更新所有路径的总概率（通常是每一步概率的对数之和）。
  3. **修剪**：然后，模型会从所有新的路径中，再次选择总概率最高的 **K** 条作为新的候选，舍弃其余的。
  4. **最终结果**：这个过程一直持续到所有路径都生成了结束标记，或者达到了预设的最大长度。最终，束搜索会从所有完成的路径中，选择总概率最高的作为最终输出。
- **图中示例**：绿色的路径代表了**束搜索找到的最佳路径**。虽然在第一个分岔路口，B 的概率（0.6）高于 A（0.4），但束搜索同时保留了 A 和 B 两条路径。在下一步，它发现沿着 A 路径的某个分支（0.9）的概率非常高，最终这使得它能够找到一个整体概率更高的路径。

总结：

- **贪婪解码**：简单，快速，但容易陷入局部最优，导致结果质量不高。
- **束搜索**：计算量更大，但通过保留多条候选路径，它能够更有效地探索潜在的序列空间，从而更有可能找到一个全局最优的输出序列，获得更高的生成质量。

![image-20250906204510211](./pic/image-20250906204510211.png)

#### exposure bias

decoder在训练中遇到的都是正确的输入，但是在测试时由于没看到过错误的输入可能造成一步错步步错的情况。

有一种解决办法是在训练时就给decoder错误的输入。这种训练方法叫**Scheduled Sampling**。

![image-20250906205027356](./pic/image-20250906205027356.png)

## 生成式对抗网络 Generative Adversarial Network GAN

### 网络作为生成器 Network as Generator

之前学的网络是只用输入一个x，输出一个y。生成式网络需要额外输入一个**Simple Distribution 简单分布**

**“Simple Distribution”（简单分布）**指的是一种数学上相对简单、容易理解和采样的概率分布。它的特点是：

- **易于理解：** 它的数学公式通常是已知的。
- **易于采样：** 我们可以用计算机算法轻松地从这个分布中生成随机样本。

使用简单分布作为输入的原因是，它为生成器提供了一个“起点”或“随机种子”。网络通过学习，将这些简单的随机数作为输入，将其“映射”或“扭曲”成一种更复杂的分布，这种复杂分布就是图片中的**“Complex Distribution”**。

简单来说，生成器利用这些简单的随机数，创造出看起来像真实数据（如图像、文本或音频）的复杂样本。例如，在生成图像时，网络可能将一个简单的随机向量（z）作为输入，然后将其转换为一张具有复杂像素模式的逼真图像（y）。

![image-20250907092613961](./pic/image-20250907092613961.png)

为什么需要分布？如下图所示，用游戏帧预测后几帧。由于训练集中小精灵有向左转也有向右转的，故预测的帧可能会出现小精灵分裂的情况，同时向左向右转。

![image-20250907093628352](./pic/image-20250907093628352.png)

这是可以加入简单分布，例如二项分布，0向左转，1向右转。

![image-20250907093803995](./pic/image-20250907093803995.png)

### 生成器与鉴别器 Generator 与 Discriminator

当需要预测的任务更有“创造性”，即可能会有很多种正确的不同输出时，会需要分布。

下图中的**正态分布**z代表从这个分布中随机抽样（一个**低维向量**），经过一个**生成器**，转化为复杂的**高维向量**。

**Complex Distribution（复杂分布）：** 这指的是真实动漫人脸图像的分布。它非常复杂，包含各种面部特征、发型、眼睛颜色等。生成器的目标就是将简单的低维向量映射到这个复杂的分布上，使得最终生成的图像（`y`）看起来像是从这个真实分布中采样的。

**Unconditional generation（无条件生成）：** “无条件”意味着生成器不需要额外的输入条件来指导它生成特定的图像。它仅仅是根据随机的低维向量`z`，随机地生成一张新的人脸。例如，你不能告诉它“生成一个金发碧眼的人”，它只会随机地生成不同特征的动漫人脸。

![image-20250908110228375](./pic/image-20250908110228375.png)

下图中是**鉴别器 Discriminator**，根据输入图像输出一个数值，值越大说明图像越像二刺螈图片。

**鉴别器是一个神经网络。鉴别器和生成器的架构由自己决定，可以是CNN也可以是transformer。**

![image-20250908110616360](./pic/image-20250908110616360.png)

### Basic Idea of GAN

如下图所示，生成器在未知参数情况下生成一组图片，鉴别器拿着真正的二刺螈图片和生成器的做对比（例如以图片中是否有黑色的眼睛作为判断标准）。并持续优化生成器参数。

当生成器进化到能够生成黑色眼睛时，鉴别器也在跟着进化，说图片中还需要判断是否有嘴巴和头发作为评判生成器生成的图片是否为真实的二刺螈图片。

类似警察和做假钞的人，做假钞需要不断提升自己的造假技术骗过警察，警察也需要不断提升鉴别能力鉴别假钞。这样协同迭代的关系叫做“**对抗 adversaria**l”

![image-20250908111341660](./pic/image-20250908111341660.png)

具体如何训练？

第一步：固定生成器，升级训练器。

生成器用随机向量生成了一堆图片。

**鉴别器（Discriminator）的工作是判断输入图片是真实的（Real）还是生成的（Fake）**。它是一个二元分类任务，因为最终的判断结果只有两种：真或假。

鉴别器既可以用分类做，也可以用回归做，原因如下：

**分类的视角（Classification）**

- **二元分类（Binary Classification）：** 这是最直观的方式。鉴别器可以被训练成一个分类模型，输出一个**离散的类别标签**。
  - **真实图片**的标签设为**1**。
  - **生成图片**的标签设为**0**。
- 训练时，鉴别器通过最小化分类错误来学习，例如使用**交叉熵损失（Cross-Entropy Loss）**。当它对真实图片输出“1”的概率很高，对生成图片输出“0”的概率很高时，损失就会很小。

**回归的视角（Regression）**

- **回归到概率分数：** 尽管任务是二元判断，但我们可以让鉴别器输出一个**连续的数值**，这个数值通常在**0到1之间**，可以被解释为“是真实图片的**概率**”。
  - **真实图片**的理想输出是**1**。
  - **生成图片**的理想输出是**0**。
- 训练时，鉴别器通过最小化预测值和目标值之间的**回归损失（Regression Loss）来学习，例如均方误差（Mean Squared Error, MSE）**。
- **为什么这样做有好处？**
  - 让鉴别器输出连续的概率值，可以提供更丰富的梯度信息。这个分数不仅告诉我们是“真”还是“假”，还告诉我们“有多真”或“有多假”。
  - 例如，一个生成的图片如果被鉴别器评分为0.6，这比被评分为0.2时更“接近真实”。这种“接近度”的梯度信息对**更新生成器G**尤其重要，因为G需要知道它生成的图片离真实图片还有多远，从而能更好地调整自己。在GANs中，鉴别器的输出分数正是用来指导生成器学习的。

![image-20250908112927788](./pic/image-20250908112927788.png)

第二步：固定鉴别器，更新生成器。

事实上，生成器和鉴别器都是多层网络，一般情况下我们把两个网络连到一块作为一个大的网络。表面上是输入一个向量，输出一个值。这个大的网络中间有一个很宽的层（生成器输出图片给鉴别器），叫做隐藏层。

在升级生成器时，鉴别器的参数不要动，要不然没意义了。

![image-20250908113536144](./pic/image-20250908113536144.png)

反复上面两个操作。

![image-20250908113859588](./pic/image-20250908113859588.png)

GAN也可以产生真实的人脸。

![image-20250908114109684](./pic/image-20250908114109684.png)

### 计算散度 

GANs的目标是训练一个生成器 G，让它能生成出与真实数据非常相似的数据。从概率分布的角度来看，就是让**生成器生成的分布（$P_G$）尽可能地接近真实数据的分布（$P_{data}$）**。

图片中间的蓝色曲线是其在一维空间上的简化表示。

下面的公式中 argmin G 表示我们要通过调整生成器 G 的参数，来找到让目标函数达到**最小值**的那个 G。

$Div (P_G,P_{data}) $代表**两个分布之间的散度（Divergence）**，它是一种衡量两个概率分布之间差异程度的数学指标。
$$
G^* = \arg \min_G \text{Div}(P_G, P_{data})
$$
这个公式与传统机器学习（如回归）不同，通过最小化**损失函数 L**，来找到最优的权重 $w^* $和偏置$ b^*$。

![image-20250908115626082](./pic/image-20250908115626082.png)

鉴别器被训练为了一个**二元分类器（binary classifier）**,目标是给真实样本打上类别1的标签，生成样本打上2的标签。

下面公式鉴别器的目标公式：需要找到一个最优鉴别器$D^*$，通过调整D的参数，最大化目标函数$V(D,G)$
$$
D^* = \arg \max_D V(D, G)
$$
**注意上面那个$G^*$公式算的是生成器的最优参数，代表生成器要最小化目标函数$Div (P_G,P_{data}) $，而这里的$D^*$算的是鉴别器的最优参数，代表鉴别器要最大化目标函数$V(G,D)$。**

其中目标函数的具体函数如下：
$$
V(G,D)=E_{y∼P_{data}} [logD(y)]+E_{y∼P_G} [log(1−D(y))]
$$

- **第一部分：$E_y∼P_{d_{data}}[logD(y)]$**

  - E 代表**期望（Expectation）**，你可以理解为对所有样本求平均。
  - $y∼P_{data}$ 表示 y 是从真实数据分布中采样的样本。
  - D(y) 是鉴别器对真实样本 y 的输出，这个值通常在0到1之间，代表它认为是真实数据的概率。
  - logD(y)：当鉴别器将真实样本正确地判断为“真”（D(y) 接近1）时，logD(y) 的值会接近 log(1)=0。当它判断错误（D(y) 接近0）时，logD(y) 的值会是一个很大的负数。为了最大化这个项，鉴别器必须努力让 D(y) 接近 1。

- **第二部分：$E_y∼P_G[log(1−D(y))]$**

  - $y∼P_G $表示 y 是从生成器分布中采样的样本。
  - D(y) 是鉴别器对生成样本 y 的输出。
  - log(1−D(y))：当鉴别器将生成样本正确地判断为“假”（D(y) 接近0）时，1−D(y) 接近1，log(1−D(y)) 接近0。当它判断错误（D(y) 接近1）时，log(1−D(y)) 是一个很大的负数。为了最大化这个项，鉴别器必须努力让 D(y) 接近 0。

  这个**V(G,D) 函数**实际上就是**二元交叉熵损失函数（Binary Cross Entropy Loss）**的负数。

![image-20250908171157041](./pic/image-20250908171157041.png)

生成器的目标函数（散度）和鉴别器的目标函数是有关联的。

从直观上说：当散度很小时，意味着生成的图片和真实的样本很接近，故鉴别器的目标函数也会很小。同理当散度很大时，鉴别器的目标函数也会很大。

![image-20250908171948315](./pic/image-20250908171948315.png)

已知两个目标函数有关系，其实可以把鉴别器的目标函数写到生成器中：
$$
D^* = \arg \max_D V(D, G) \\
G^* = \arg \min_G \text{Div}(P_G, P_{data}) = \arg \min_G \max_D V(D, G) \\
$$
![image-20250908172620108](./pic/image-20250908172620108.png)

### 训练GAN

为什么在实际训练中，用**JS（Jensen-Shannon）散度**来衡量生成器分布$P_G$和真实数据分布$P_{data}$之间的差异是行不通的。

在实际训练中，真实图片和训练图片的数据分布（$P_G$和$P_{data}$）往往重合度较少

把图片类比为平面中的两个曲线，两个曲线重合或相似的部分其实很少。**当两个分布没有重叠时，基于JS散度的梯度会变得非常小，甚至为零，导致生成器无法得到有效的训练信号**。它不知道该往哪个方向走才能使自己生成的图片更像真实的图片。

在判断真实图片和训练图片时，由于样本数量（**注意这里的样本指的是从整个分布（所有图片）中的部分图片**）有限，我们很可能**根本抽不到重叠部分的样本**。这会导致**鉴别器过于自信**，其损失迅速降为零，再次导致**梯度消失**，生成器得不到任何有用的反馈来改进。

![image-20250908174208631](./pic/image-20250908174208631.png)

当两个分布不重合时，JS 散度总是最大值$log2$，对于loss和改进GAN没有帮助。

当生成器生成的图片和真实图片在特征上完全没有重叠时，鉴别器（一个二元分类器）可以非常容易地把它们区分开来（**因为只要不完全重叠就是$log2$ **）。这就像把苹果和橙子放在一起，鉴别器可以轻而易举地100%正确地分类，它的**分类准确率达到了100%**。

当鉴别器的loss趋近于0时，就失去了**梯度**，没办法告诉生成器应该往哪个方向努力，**生成器不知道该如何改进**。

![image-20250908174729668](./pic/image-20250908174729668.png)

**Wasserstein距离**的定义：Wasserstein距离衡量的是，将一堆土从一个分布形状（P）移动到另一个分布形状（Q）所需的**最小平均运输距离或成本**。

与JS散度的区别：

- **JS散度**：只关心两个分布是否重叠。如果它们不重叠，JS散度就达到最大值，无法给出任何梯度信息，就好比告诉你“两个地方不挨着，但不知道它们到底有多远”。
- **Wasserstein距离**：即使两个分布没有重叠，它也能**提供有意义的距离度量**。就好比图中，无论 P 和 Q 相隔多远，它们之间的距离 `d` 总是存在的，并且可以计算出来。这个距离值可以作为**梯度**，告诉生成器应该朝着哪个方向移动，才能让自己的分布更接近真实分布。

![image-20250908175831416](./pic/image-20250908175831416.png)

计算Wasserstein 距离需要**穷举**所有的推土方法，找到一个平均移动距离最小的方案作为Wasserstein 距离。

![image-20250908180102904](./pic/image-20250908180102904.png)

Wasserstein 距离可以随着分布的接近而变小。

![image-20250908180340850](./pic/image-20250908180340850.png)

### WGAN

用Wasserstein计算分布之间的散度的GAN叫WGAN，下面是计算公式。生成器希望值越小越好，所以表达为负期望；鉴别器希望值越大越好，所以表达为正期望。WGAN的鉴别器不再输出0或1的概率，而是输出一个**任意的实数**。
$$
\max_{D \in 1-Lipschitz} \{ \mathbb{E}_{y \sim P_{data}}[D(y)] - \mathbb{E}_{y \sim P_G}[D(y)] \}
$$
 其中，D(y) 是鉴别器对样本 y 的输出。

**D∈1−Lipschitz**：这是WGAN的关键创新，它对鉴别器D施加了**1-Lipschitz连续性约束**。这意味着鉴别器函数的斜率不能超过1，从而保证其**平滑性（smooth enough）**。

这个图示解释了如果没有Lipschitz约束会发生什么。

- **横轴**代表了数据空间（比如一维的向量）。
- **绿点**代表**真实数据**的样本。
- **蓝点**代表**生成数据**的样本。
- **红色的曲线**代表没有约束的**鉴别器函数 D**。
- **问题所在：** 如果没有Lipschitz约束，鉴别器可以变得**非常陡峭**。它会把真实数据所在的区域拉到**正无穷**，把生成数据所在的区域推向**负无穷**。
- **结果：** 鉴别器会变得过于强大，它的输出值（评分）会变得无限大。这导致它的**训练无法收敛**，也无法为生成器提供稳定的、有意义的梯度，整个系统再次崩溃。

![image-20250908193032177](./pic/image-20250908193032177.png)

这张PPT承接了上一页WGAN的讨论，重点讲解了**如何实际地实现对鉴别器（Critic）的“1-Lipschitz连续性”约束**，以及WGAN是如何从最初的版本演化到更优的实现的。

目前主要用**谱归一化（Spectral Normalization）**

* **方法：** 这是另一种实现Lipschitz约束的方法，常用于最新的GAN模型中。

* **核心思想：** 它通过**约束网络层的权重矩阵的谱范数（spectral norm）**来保证梯度范数不会超过1。

* **目的：** 这种方法不像梯度惩罚那样需要额外的采样和计算，**计算效率更高**，而且效果也非常好。

![image-20250908193523909](./pic/image-20250908193523909.png)

在序列生成上使用GAN是很困难的一件事。

当解码器生成文字时，是根据某一个token（token是生成序列的最小单位，这里这的是输出的单个向量）最接近哪个文字向量判断的。然而GAN的鉴别器在告诉生成器梯度的时候，生成器会调整参数，但输出的单个向量仍然可能最接近上一次的文字。故鉴别器分数不变，生成器也无法继续优化。

在传统的GANs中，生成器 G 生成的是连续的、可微的数据（比如图像的像素值），这使得我们可以计算损失函数的梯度，然后用梯度下降来更新生成器。

但是，生成**序列数据**（如文本中的词语或音频中的音符）是一个**离散**的过程。

- 生成器 G 的输出是一个个**离散的符号**（例如，一个词语的 ID）。
- 鉴别器 D 接收这些**离散的序列**并给出评分。

这就是**梯度不可导（Non-differentiable）**问题。

![image-20250908195327377](./pic/image-20250908195327377.png)

### GAN评估

如何评价生成出来的图片？

我们可以把一张图片丢到影像辨识系统中，会输出一个概率分布。分布越集中越好。

![image-20250908200812269](./pic/image-20250908200812269.png)

但是用影像分类会产生**模式崩溃 Mode Collapse**的问题。**模式崩溃**指的是生成器无法产生多样化的数据，而是倾向于**重复生成少数几种甚至同一种类型的样本**。它只学习了真实数据分布中的一小部分，忽略了其余部分，从而失去了生成多样化样本的能力。

比如生成器在生成某一个图片之后会骗过鉴别器，然后就会一直生产和这个图片相似的图片。但是很难生产其他图片。

![image-20250908201152007](./pic/image-20250908201152007.png)

**模式丢失（Mode Dropping）**。这张图片展示了生成器在训练过程中，**如何“丢失”或“忽略”真实数据分布中的某些模式（多样性）**。

* `Generator at iteration t`：在训练的某个阶段 `t`，生成器还能够生成一些多样化的人脸（虽然有些质量不高）。

* `Generator at iteration t+1`：仅仅经过一次迭代后，生成器就迅速“放弃”了它之前学习的某些模式，转而生成与少数几个人脸模板（比如左数第一、二、三列的女性脸）相似的图片。

这表明，**GANs很难同时学习到所有的数据模式，尤其是在面对高度多样化的数据集时。**

![image-20250908201820994](./pic/image-20250908201820994.png)

生成一组图片，如果经过图片分配器识别的分布都很集中，则表明多样性很差。

如果分布很平坦，则多样性很好。

复习概率论：

* **条件概率（Conditional Probability）**：P(A∣B) 表示在事件 B 发生的前提下，事件 A 发生的概率。在图中，$P(c∣y^n)$ 表示给定一张图片$ y^n$，它属于类别 c 的概率。

* **期望（Expectation）或平均值**：是对一个随机变量所有可能取值的一个加权平均，权重就是每个取值的概率。在图中的公式里，$1/N ∑P(c∣y^n)$ 实际上就是对所有生成样本的概率分布求平均。

![image-20250908202205550](./pic/image-20250908202205550.png)

![image-20250908202239201](./pic/image-20250908202239201.png)

评价图片的方法：

**IS分数：** **Inception Score (IS)** 是衡量生成模型质量和多样性的一个重要指标。

**高IS分数的含义：**

- **高质量（Good quality）：** 意味着生成的图片清晰、逼真，并且分类器能自信地将它们归为某一类。
- **高多样性（Large diversity）：** 意味着生成的图片种类丰富，能覆盖不同的模式，而不仅仅是重复生成少数几种图片。

但是对于动漫头像，图片识别器可能识别的都是人脸，故IS可能会在多样性上面打低分。

**FID（Fréchet Inception Distance）**的评估方式与IS不同，它解决了IS的这一缺陷。FID不是简单地计算类别分布的均匀性，而是直接比较两个分布的相似度。

- **红色的点**代表**真实图片**在高维特征空间中的分布。
- **蓝色的点**代表**生成的图片**在高维特征空间中的分布。
- **FID计算的是这两个分布之间的“距离”**。这个距离越小，代表生成图片和真实图片在质量和多样性上越接近。

![image-20250908203616298](./pic/image-20250908203616298.png)

还有一个问题，如果GAN把真实照片直接背下来输出，那么FID的值也会很小。如何避免这种情况？

如果再加一个判断生成图片和真实数据相似度的比较，那么GAN生成真实图片的反转图片又可以避免这种检测。

目前研究中。

### 条件式生成 Conditional Generation

可以输入特定条件。生成不同的图片因为每次从z中拿出的向量不同。

![image-20250908205428087](./pic/image-20250908205428087.png)

如果鉴别器不能根据输入的条件识别图片，则最终生成器也会忽略输入的条件。

在实际训练中，鉴别器也需要输入条件。只有当图片和条件对的上才会打高分，而图片模糊或者图片和文字对不上都会打低分。

![image-20250908210731364](./pic/image-20250908210731364.png)

条件不止可以是文字，也可以是图片。例如输入线稿生成图片。

![image-20250908210810212](./pic/image-20250908210810212.png)

![image-20250908213644806](./pic/image-20250908213644806.png)

也可以输入声音输出图片。

![image-20250908213925645](./pic/image-20250908213925645.png)

### 从非配对数据中学习 Learning from Unpaired Data

 GAN 怎么用在Unsupervised 学习上。

训练数据集中，如果输入和输出都不成对应该怎么训练模型？

例如影像风格转换，没有任何成对资料。

![image-20250908214651894](./pic/image-20250908214651894.png)

解决办法如下，只要生成器生成的图片经过鉴别器判断是否匹配二刺螈图片，就可以训练风格转化。

但是如果生成器仅用随机向量生成一个二次元图片而忽略输入的图像也可能被鉴别器打高分。

![image-20250908214820356](./pic/image-20250908214820356.png)

为了避免上述问题，额外需要一个生成器（$G_{y→x}$）根据生成图片再转化为原来风格的图片。

由于这是一个循环，所以也叫做**Cycle GAN**，核心思想是**无监督图像到图像的双向映射**

这样的GAN也存在一个问题，如果生成器只是简单的将图片反转再反转，则同样也能保持一致性。这个问题到目前为止还在探讨解决方案。

![image-20250908215622822](./pic/image-20250908215622822.png)

同时，我们也需要训练（$G_{y→x}$），让他能够把动漫人脸变为真实的人脸，并用（$G_{x→y}$）生成回去。然后把真实人脸给$D_x$鉴别。

问题：

比如任意的真实人脸给$G_{x→y}$都能生成银时，$G_{y→x}$也能生成和前面一样的真实人脸。 

 当银时再给$G_{y→x}$时，也会生成和上面一样的真实人脸，给$D_x$也会打高分。 

 那这样也还是会造成模式崩溃（失去多样性）

回答：

1. **鉴别器 DX 的博弈**：

   - 你的假设是 DX 会给银时转换来的真实人脸打高分。但请记住，**鉴别器也在持续学习**。

   - 如果 GX→Y 确实只生成银时，那 GY→X 就会不断地将银时转换成各种假人脸。这些假人脸虽然在**局部**看起来像真实人脸，但它们都源自同一个**单一的输入**。

   - 鉴别器 DX 会很快意识到，它看到的这些“真实人脸”都**缺乏多样性**。它们在特征空间中会聚集在一个很小的区域，而真正的真实人脸（来自训练集）则分布得非常广。

   - 聪明的 DX 会学会识别这种“非多样性”的假人脸，并开始给它们打低分。这会迫使 GY→X 做出改变，从而反向推动 GX→Y 也必须改变。

2. **循环一致性的作用**：

   - 循环一致性损失是一个**强约束**，它要求**整个转换过程是可逆的**。

   - 如果你将所有真实人脸都映射到了一个单一的银时头像，这就意味着**多种不同的输入**（你、我、他）都映射到了**同一个中间状态**。从这个中间状态**再想恢复出所有不同的原始人脸**，这在数学上是非常困难的。

   - 尽管理论上可行，但神经网络要实现如此复杂的非线性映射（多对一再一对多），其难度远高于直接学习一个有意义的、多对多的映射。

3. **训练的随机性和梯度**：

   - 训练开始时，模型是随机初始化的。它不会一开始就找到这种“作弊”的捷径。

   - 随着训练进行，模型会**同时**优化多个损失函数（两个对抗损失和两个循环一致性损失）。这些损失函数共同作用，互相制约，就像多根绳子在拉着模型。

   - 最终，模型会找到一个**最容易同时满足所有损失函数**的平衡点，而这个点通常就是**学习一个有意义的、保持多样性的转换**。

![image-20250908220856087](./pic/image-20250908220856087.png)

cycle GAN也可以做文字风格转换的功能，让长文章转换为精简的摘要，把中文翻译成英文，语音辨识...

## 自监督学习 Self-Supervised Learning

监督学习 (Supervised Learning)

监督学习就像一个有老师指导的学生。训练数据中，**每个样本都有一个正确的答案（标签）**。模型通过学习这些数据和它们对应的标签之间的关系，来学会预测新数据的标签。

无监督学习 (Unsupervised Learning)

无监督学习则像一个没有老师的学生。训练数据**没有标签**，模型需要自己去发现数据中隐藏的模式、结构或关系。它主要用于数据探索和模式识别。

**自督导式学习也是无监督学习**，因为没有标签。这里强调自督导式学习只是明确类型。

![image-20250909091756086](./pic/image-20250909091756086.png)

### BERT

**BERT** (Bidirectional Encoder Representations from Transformers) 是一种**预训练语言模型**，由 Google 在 2018 年发布。简单来说，它是一种能**理解人类语言上下文**的强大工具。

BERT 的核心创新在于它的**双向性（Bidirectional）**。在 BERT 之前，大多数语言模型都是**单向**的，它们从左到右或从右到左地处理文本。这就像只读一个句子的前半部分或后半部分来猜测中间的词，会丢失重要的上下文信息。

- **单向模型**：在句子“我把钱存在了**银行**里”中，如果模型只从左向右读，它在看到“银行”这个词之前，只知道“我把钱存在了”。
- **BERT 的双向性**：BERT 可以同时查看“我把钱存在了”和“里”，从而准确地理解“银行”在这里指的是金融机构，而不是河岸。

这种**双向**处理的能力让 BERT 能更深入地理解一个词在句子中的真正含义，因为它同时考虑了左边和右边的所有词。

**BERT 的工作原理**

BERT 的工作流程分为两个主要步骤：

1. **预训练 (Pre-training)**： BERT 首先在一个巨大的文本语料库（如维基百科和书籍）上进行训练。在这个阶段，它会进行两项核心任务来学习语言的通用模式：
   - **掩码语言模型（Masked Language Model）**：模型会随机“遮住”句子中的一些词，然后预测这些被遮住的词是什么。这就像一个完形填空游戏，强迫模型理解句子的上下文。
   - **下一句预测（Next Sentence Prediction）**：模型会同时看两个句子，然后预测第二个句子是否是第一个句子的下一句。这帮助模型理解句子之间的关系。
2. **微调 (Fine-tuning)**： 预训练完成后，BERT 模型已经对语言有了非常好的理解。接下来，你可以用一个特定任务的数据集（比如问答、情感分析）来对它进行**微调**。你只需要在 BERT 模型上加一个简单的输出层，就可以让它迅速适应新的任务，而不需要从头开始训练一个全新的模型。

在预训练中，如下图所示，类似transformer的编码器，可以用一个mask（一个特别的token）盖住某一个字，也可以随机的把某个字替换成其他字。进行训练，输出一个向量和真实的文字做比较。

虽然BERT在训练时看起来像是在“监督”下学习（因为它有目标），但这个“监督”是**自动**从无标签的数据中提取出来的。这使得 BERT 能够利用几乎无限量的文本数据进行训练，而**不需要昂贵的人工标注。**

![image-20250909092751001](./pic/image-20250909092751001.png)

如下图所示，BERT会读入两个句子。输入yes或no表示两个句子是否有顺序关系。但是这个Next Sentence Prediction不是很有用。

![image-20250909093226025](./pic/image-20250909093226025.png)

如下图所示，BERT在预训练之后，经过**微调（Fine-tune）**可以做各种各样的工作。

![image-20250909093745668](./pic/image-20250909093745668.png)

一般来说，测试自监督学习需要微调成多个模型然后分别测试。例如将BERT分化为多个任务，给每一个任务都打分，然后取平均。

![image-20250909093937167](./pic/image-20250909093937167.png)

### BERT 的使用案例

#### 判断句子情感

下图中，整个模型由两个部分组成，一个是BERT，还有一个是线性层。

为什么生成的线性层要随机初始化？

图片中BERT模型上方连接的**Linear（线性层）**是专门为**情感分析**这个任务添加的，它需要从头开始学习。

- **BERT和线性层的分工**：
  - **BERT**：它是一个强大的**特征提取器**，已经通过预训练学会了如何将复杂的文本信息转化为有意义的向量表示（图中`[CLS]`标记对应的向量）。
  - **线性层**：它的任务很简单，就是接收BERT提取的向量（特征），然后将它**映射**到最终的输出结果（比如“积极”或“消极”）。
- **为何随机初始化**：这个线性层是一个全新的、从零开始的模块，它没有经过任何预训练。因此，它没有任何先验知识，只能**随机初始化**其参数。在微调（Fine-tuning）过程中，这个线性层会和BERT模型一起，利用有标签的情感分析数据（`this is good` -> `positive`）来学习如何将BERT的特征向量正确地分类。

BERT的强大之处就在于，它已经完成了大部分的“理解”工作，新的线性层只需要学习一个相对简单的**映射**关系，就能在短时间内达到非常好的效果。这就像是给了学生一个已经整理好的知识框架，他只需要在这个框架上添加一些新的知识点（线性层），就可以完成特定的考试任务。

注意BERT在微调之后用于特定任务时，仍然需要有标注资料训练。所以BERT也被称为**semi-supervised 半监督学习**。

![image-20250909094929764](./pic/image-20250909094929764.png)

有预训练的BERT比随机初始化的BERT在测试集上效果好

![image-20250909095040272](./pic/image-20250909095040272.png)

#### 词性标注

BERT可以用于词性标注

输入几个字（token），输出几个向量，给线性层之后再softmax一下做分类。

![image-20250909095727231](./pic/image-20250909095727231.png)

#### 自然语言推理 Natural Language Inference

输入一个前提，输入一个假设，判断假设和前提是否相符合。可以用于**判断文章立场**。

![image-20250909095919256](./pic/image-20250909095919256.png)

同样这是一个分类任务，BERT需要预训练后再训练这个。

![image-20250909100135565](./pic/image-20250909100135565.png)

#### 抽取式问答 Extractive Question Answering

答案出现在文章里边的问答。

如下图所示，D和Q指的是文章和问题的序列，s和e指的是在文章中答案的起止位置。

![image-20250909100516220](./pic/image-20250909100516220.png)

为什么要用随机生成的向量和BERT得到的向量做内积得到答案的起始位置，随机生成的向量和输入的问题序列没有关系吗？

* **直接关系**：起始向量和结束向量本身与问题序列的词向量**没有直接的运算或关联**。

* **间接关系**：BERT模型本身在处理输入时，会将问题序列（`q1, q2`）和文档序列（`d1, d2, d3`）**拼接**在一起，并在中间用 `[SEP]` 标记分隔。BERT的**自注意力机制**（Self-Attention）会同时考虑问题和文档的所有词语，让文档中的每个词向量（如 `d1`）都**隐式地融入了问题的语义信息**。

因此，当起始向量（或结束向量）与文档中的词向量做内积时，它实际上是在评估“这个词**在考虑到问题的前提下**，有多大可能是答案的起始”。

![image-20250909100846161](./pic/image-20250909100846161.png)

### 训练BERT

自己训练BERT很难。

![image-20250909102042322](./pic/image-20250909102042322.png)

### BERT为什么有用

BERT考虑了上下文，例如吃苹果和苹果手机的果含义不一样，那么BERT输出的向量也不一样。

![image-20250909102451492](./pic/image-20250909102451492.png)

下图是用cos计算了每一个句子之间“果”向量的相似度。

![image-20250909103000744](./pic/image-20250909103000744.png)

 BERT可以根据上下文推测出被盖住的文字本应输出的向量。事实上，在BERT之前就有词嵌入技术也可以做这个，但是局限性在于一个字或者词出现在不同的句子有着不同的含义情况下，仍然会输出相同的向量。简单来说，BERT 解决了传统词嵌入技术的**一词多义**问题。

![image-20250909103508470](./pic/image-20250909103508470.png)

#### **BERT 的核心能力和局限性**

BERT之所以强大，恰恰在于它能捕捉到序列中的**深层规律和模式**。它通过处理大量的文本数据，学习到了语言的语法、语义和上下文关系，这些都是高度有规律的。

- **如果序列没有规律**：例如，一个完全随机生成的字母或数字序列，BERT 的**预训练**将毫无用处。因为它无法从这些无序数据中学习到任何有用的模式。BERT会发现“掩盖”某个元素后，它的上下文无法提供任何有意义的线索来预测它是什么。

这张PPT想表达的是：**即使在非语言领域，只要数据具有类似语言的“序列”和“上下文”结构，BERT也能发挥作用。**

- **表格中的数据**：表格展示了 BERT 被应用于蛋白质（Protein）、DNA 和音乐（Music）等领域的分类任务。这些领域的数据虽然不是自然语言，但它们都是**有序列、有结构的**。
  - **蛋白质**：由氨基酸序列组成。
  - **DNA**：由核苷酸序列（A, T, C, G）组成。
  - **音乐**：由音符序列、和弦序列组成。
- **BERT 的优势**：
  - **BERT**：表中BERT行的分数通常最高。这表明BERT在这些任务中表现出色，因为它能够像处理语言一样，捕捉到序列中元素（如氨基酸、核苷酸）之间的**长距离依赖关系和深层结构**。
  - **rand**：这行代表使用随机初始化的模型。它的分数最低，因为它没有经过预训练，无法理解序列中的任何模式。
  - **re-emb**：这行代表重新嵌入。

![image-20250909104150231](./pic/image-20250909104150231.png)

#### 多语言BERT Multi-lingual BERT

![image-20250909104258137](./pic/image-20250909104258137.png)

给英文资料多语言BERT会做中文的问答。

多语言BERT（Multi-lingual BERT）有强大能力，特别是它的**“零样本学习”（Zero-shot Learning）**能力。

`Pre-train: 104 languages` -> `Fine-tune: English` -> `Test: Chinese`

- 这正是这张PPT最核心、最惊人的发现。**“零样本学习”**指的是模型在某个任务上（这里是问答）**没有见过任何中文数据**，但仍然能在中文测试集上进行预测。
- 具体过程：模型在104种语言上预训练，然后只用**英语问答数据**进行微调，最后直接拿去回答**中文**问题。
- 结果：EM 63.3, F1 78.8。尽管分数有所下降，但这个结果依然非常了不起。这说明多语言BERT学到的不是单纯的语言，而是一种**跨语言的、通用的语言表示**。它能够将中文问题映射到它从英语中学到的知识上，并尝试给出答案。

![image-20250909105007527](./pic/image-20250909105007527.png)

也许多语言BERT在看过大量语言过后，学会了不同语言相同意思的词汇做成相似的向量。但是需要在足够大量的资料的基础上。

给多语言BERT输入英文会生成一组向量，再加上英文和中文词向量的平均差距，生成出来的就是和英文对应的中文句子。

![image-20250909105925583](./pic/image-20250909105925583.png)

![image-20250909110125376](./pic/image-20250909110125376.png)

### GPT

GPT更像是一个解码器，预测下一个词汇是什么。但是类似掩码自注意力，也无法看到下一个预测的词汇。

![image-20250909110400247](./pic/image-20250909110400247.png)

大型语言模型（LLM）的**“上下文学习”（In-context Learning）**能力

上下文学习（In-context Learning）是一种不同于传统机器学习微调（fine-tuning）的学习方式。在传统微调中，你需要用特定的任务数据来更新模型的参数（通过梯度下降）。而**上下文学习**则不需要更新模型的参数。

它通过在**输入提示（prompt）中提供一些示例（examples）**，来引导模型理解并完成任务。模型就像一个聪明的学生，通过看几个例子就能学会解题方法。

三种学习模式。PPT通过一个“英译法”的任务，生动地展示了这三种学习模式的区别：

1. 零样本学习（Zero-shot Learning）

   - **输入**：模型只接收到任务描述（“Translate English to French”）和要翻译的词（“cheese”）。

   - **学习方式**：没有提供任何示例。模型必须完全依赖其**预训练中学到的知识**来完成任务。

   - **表现**：如果模型在预训练时见过类似的翻译任务或数据，它就能直接给出答案。

2. 单样本学习（One-shot Learning）

   - **输入**：模型接收到任务描述、**一个示例**（“sea otter => loutre de mer”）和要翻译的词。

   - **学习方式**：通过这一个例子，模型可以推断出任务的格式和具体要求（例如，输入和输出之间用“=>”连接）。这有助于模型更好地理解任务。

   - **表现**：通常比零样本学习效果更好，因为模型有了一个明确的“样板”可以遵循。

3. 少样本学习（Few-shot Learning）

   - **输入**：模型接收到任务描述和**多个示例**（“sea otter => loutre de mer”，“peppermint => menthe poivrée”，“plush giraffe => girafe peluche”），以及要翻译的词。

   - **学习方式**：通过更多的例子，模型能够更准确地理解任务的模式、风格和潜在的复杂性，减少误解。

   - **表现**：通常是这三种模式中效果最好的，因为更多的示例为模型提供了更强的上下文引导。

![image-20250909111045482](./pic/image-20250909111045482.png)

## 自编码器 Auto-encoder

自编码器是一种具体的模型，自监督学习是一种训练方法。自编码器**是自监督学习的一个典型例子**。

自编码器是一种特殊的**神经网络**，它的主要任务是**无监督**地学习数据的有效表示或**编码（encoding）**。它由两个主要部分组成：

1. **编码器（Encoder）**：它负责将原始输入数据（例如一张高维度的图片）压缩成一个低维度的**向量（vector）**，这个向量也叫做**编码（code）、嵌入（embedding）或特征表示（representation）**。这个过程就像是把一篇文章的核心思想提炼成一个精炼的摘要。
2. **解码器（Decoder）**：它负责接收这个低维度的向量，并尝试**重构（reconstruction）**出与原始输入尽可能相似的输出。这个过程就像是根据摘要，尝试还原出完整的文章。

为什么说它能用于“降维”？

如图所示，编码器和解码器之间的那个低维度向量被称为**“瓶颈”（bottleneck）**。这个瓶颈是自编码器工作的核心。

- 为了让解码器能够成功地重构出原始输入，编码器必须学习如何将最重要的信息压缩到这个小小的瓶颈向量中。
- 这个压缩过程本质上就是**维度缩减（Dimension reduction）**。它丢弃了数据中冗余、不重要的信息，只保留了最核心的特征。

通过这个过程，我们得到的低维向量（`vector`）就是原始数据的一个**“新特征”**，它比原始高维度数据更紧凑，但保留了关键信息，因此可以用于后续的任务。

![image-20250909185407959](./pic/image-20250909185407959.png)

输入的图片中，尽管很复杂，但是仍然可以分辨特征转化为低维向量。例如下面所示，尽管图片是3*3大小，但是所有图片只有两种分布，故可以用一个二维向量表示。

我们用这个二维向量给后续处理可以减小计算量。

![image-20250909185825695](./pic/image-20250909185825695.png)

#### 去噪自编码器 De-noising Auto-encoder

去噪自编码器的核心思想是在训练时，给原始数据**人为地加入一些噪声**。然后，模型需要学会从这个带噪声的版本中，恢复出**干净的、原始的数据**。

这种训练方式迫使模型学习更鲁棒的、更本质的数据表示。

- **普通自编码器**：如果输入和输出都是完全一样的干净数据，模型可能会学到一些非常浅显的、不重要的特征来完成重构任务。
- **去噪自编码器**：当数据被噪声污染时，模型不能依赖那些不重要的特征，因为它必须过滤掉这些噪声才能找到数据的真正核心特征。这就像是在听一首嘈杂的歌曲时，你必须学会忽略噪音，才能分辨出真正的旋律。

![image-20250909190605893](./pic/image-20250909190605893.png)

如下图所示，其实BERT也可以看作是一个去噪自编码器。因为需要做完形填空，空白就是人为加上的噪声。

![image-20250909190904340](./pic/image-20250909190904340.png)

#### Feature Disentangle 功能区分

例如自编码器读取了一段语音，转化为一个向量（Embedding）。那么如何识别这个向量中哪些是有关于读者的信息，那些是关于内容的信息呢？

![image-20250909191507698](./pic/image-20250909191507698.png)

应用：声音转换。

功能区分可以分辨出向量哪一部分是说话者，语音转化就是将这一部分替换为目标向量的部分。

不需要两段语音说一样的内容。

![image-20250909191848772](./pic/image-20250909191848772.png)

![image-20250909192344333](./pic/image-20250909192344333.png)

中间这个embedding可以是二元向量也可以是独热向量。

![image-20250909195426850](./pic/image-20250909195426850.png)

**向量量化变分自编码器（Vector Quantized Variational Auto-encoder, VQ-VAE）**。它解决了一个核心问题：如何将连续的特征向量（continuous vector）转化为**离散的（discrete）**表示。

* **码本（Codebook）**：VQ-VAE 引入了一个特殊的**码本**，它是一个由一组离散的、可学习的向量组成的“字典”。图中的 `vector 1` 到 `vector 5` 就是码本中的向量。这些向量是模型从数据中学习到的。

* **向量量化（Vector Quantization）**：这是 VQ-VAE 的核心步骤。编码器输出的连续向量**不会直接传递给解码器**。相反，模型会计算这个连续向量与**码本中所有向量的相似度**（例如使用余弦相似度或L2距离）。

* **选择最相似向量**：模型会从码本中**选择**与编码器输出最相似的那个向量。这个被选中的向量（图中的 `vector 3`）才是真正传递给解码器的输入。

在传统的自编码器中，编码器输出的是一个连续的向量。但在某些应用场景中，我们需要离散的编码。例如：

- **自然语言处理**：语言中的词语、音素等都是离散的符号。
- **图像生成**：将图像分解成离散的“视觉单词”或“块”，可以更好地理解和生成图像。

![image-20250909200013287](./pic/image-20250909200013287.png)

从自编码器角度看cycle GAN

例如生成器读入文章生成一段摘要，解码器再把这段摘要还原为原来的文章。

但是人是看不懂这段中间生成的摘要的，需要一个鉴别器判断这是否为人写的摘要。

反过来会改进生成器。

![image-20250909200425963](./pic/image-20250909200425963.png)

![image-20250909200701712](./pic/image-20250909200701712.png)

 其他应用：

比如解码器可以拿来做生成器。

![image-20250909201452931](./pic/image-20250909201452931.png)

也可以压缩图片。尽管解码器做的是解压缩，但是在某些方法下图片仍然会失真。

![image-20250909201615667](./pic/image-20250909201615667.png)

#### 异常检测 Anomaly Detection

判断一个东西是否异常取决于训练资料长什么样子，是相对的。

![image-20250909201759719](./pic/image-20250909201759719.png)

例如交易欺诈，网络判断，癌细胞检测等

为什么不直接训练一个二元分类器（binary-classifier）?因为正常数据很好找，异常数据很难找，很难训练机器判断什么是异常。

当只有正常的资料时，这就是自编码器排上用场的时候了。

比如用真实人脸训练自编码器，一个测试的真实人脸编码再解码之后会看到类似的人脸。

然而用二刺螈人脸测试时，由于自编码器没有见过这种东西，所以输出的不是真实的人脸。

需要通过**reconstruction loss 重构损失** 判断输入数据是否异常。

![image-20250909202429788](./pic/image-20250909202429788.png)

## 对抗攻击 Adversarial Attack

 神经网络也需要防止攻击。

如下图所示，我们要加上人肉眼识别不到的噪音，攻击有两种类型，一种是非定向攻击（不要输出猫），一种是定向攻击（输出海星）

![image-20250910112838800](./pic/image-20250910112838800.png)

左边是**正常图像（Benign Image）**，右边是**对抗图像（Attacked Image）**。

补充：

ResNet-50 是一种非常流行的**卷积神经网络**（CNN）模型，专门用于图像识别和分类任务。

ResNet 的全称是 **Residual Network**（残差网络）。它的核心创新在于引入了“**残差块**”（Residual Block）的概念。

在传统的深度神经网络中，层数越多，理论上模型的表现力就越强。但实际上，当网络层数增加到一定程度时，模型会出现“**退化**”问题——性能不升反降，并且训练变得非常困难。

为了解决这个问题，ResNet 引入了**跳跃连接**（Skip Connection），也就是在网络中加入一个“捷径”。这个捷径允许信息直接跳过一两层，将输入直接加到后面的输出上，形成一个残差学习单元。这样，网络就不再学习从输入到输出的全部映射，而是学习输入和输出之间的**残差**（即差异），从而大大简化了训练过程，并允许网络构建得更深。

这里的 **“50”** 指的是模型中可训练的**网络层数**。除了 ResNet-50，还有 ResNet-18、ResNet-34、ResNet-101、ResNet-152 等不同版本，数字越大，网络越深，参数也越多，通常性能也越强，但计算成本也越高。

ResNet-50 因为其在性能和计算效率之间找到了很好的平衡，成为了计算机视觉领域一个非常常用的基准模型。

![image-20250910113052884](./pic/image-20250910113052884.png)

如果只是加入了一般的噪音，模型是能够粗略辨别出这是一只猫。

![image-20250910114120883](./pic/image-20250910114120883.png)

正常的神经网络在读入图片会输出图片可能是什么东西的分布。

非定向攻击的目标是只要输出分布的猫的值越小越好。我们需要找到一组参数，使得非定向攻击图片的loss越小越好。且对抗图片和原来的图片的参数值需要控制在一个范围内，不能让人眼识别出来。

注意这里的Loss是生成分布和猫分布交叉熵的负值，越小代表着生成分布和猫分布交叉熵越大。
$$
x^* = arg \min_{d(x^0, x) \leq \epsilon} L(x) \\
L(x) = -e(y, \hat{y})
$$
同理，定向攻击的既需要与猫的交叉熵很大，有需要与目标的交叉熵很小，所以loss计算公式如下：
$$
L(x) = -e(y, \hat{y}) + e(y, y^{target})
$$


![image-20250910115107750](./pic/image-20250910115107750.png)

在对抗攻击中，有一个关键的约束条件：**对抗扰动是肉眼不能识别的**。对抗图像 `x` 可以看作是原始图像 `x⁰` 加上一个微小的扰动 `Δx`。

有两种计算方法：

1. L2 范数 (L2-norm)

   - **数学表示**：d(x0,x)=∣∣Δx∣∣2

   - **计算方式**：它等于扰动向量 `Δx` 中所有元素平方和的平方根。

   - **直观理解**：L2 范数衡量的是扰动**整体的“能量”或“幅度”**。如果把图像看作一个高维空间中的点，L2 范数就是原始图像和对抗图像在欧几里得空间中的距离。如果这个距离很小，就意味着两张图整体上非常相似。

2. L-无穷范数 (L-infinity norm)

   - **数学表示**：d(x0,x)=∣∣Δx∣∣∞

   - **计算方式**：它等于扰动向量 `Δx` 中所有元素绝对值的最大值。

   - **直观理解**：L-无穷范数衡量的是扰动**在任何一个像素点上的最大变化量**。如果这个最大变化量很小，就意味着图像中的任何一个像素点都不会有很大的改变。这种方式确保了扰动不会在某个像素上显得特别突兀。

   在实际中，无穷范数更能够判断人眼是否能够识别对抗图像和正常图片的区别。

   ![image-20250910121446238](./pic/image-20250910121446238.png)

   在下一页中，对抗攻击最小化Loss并不调整参数，而是调整输入图像。

   幻灯片的左下角展示了最常用的攻击方法：**梯度下降（Gradient Descent）**。

   这个过程和训练神经网络非常相似，但角色互换了：

   1. **初始化：** 从原始图像 `x⁰` 开始。
   2. **迭代（t = 1 到 T）：** 在每一次迭代中，我们计算当前图像 `xᵗ⁻¹` 关于损失函数 `L(x)` 的**梯度 `g`**。
   3. **更新：** 沿着梯度的方向更新图像，得到新的图像 `xᵗ`。这里的 `η` 是学习率，控制每次更新的步长。
      - **非定向攻击**：梯度 `g` 指向让损失函数值变小的方向，也就是**远离正确分类的方向**。我们沿着这个方向更新图像，以达到误分类的目的。
      - **定向攻击**：梯度 `g` 同时考虑了远离正确分类和靠近目标分类的方向，确保最终结果是目标类别。
   4. **约束：** 每次更新后，检查新图像 `xᵗ` 是否仍然满足与 `x⁰` 的距离小于 `ε` 的约束。如果超过了，就把它“投影”回约束范围内，确保扰动始终不可见。

   以 L-infinity 范数为例

   右下角的图用 **L-infinity** 范数（L-∞）直观地展示了这个过程：

   - **黄色点 (`x⁰`)**：原始图像，位于一个以 `x⁰` 为中心、边长为 `2ε` 的正方形区域内。这个正方形代表了所有可以接受的、肉眼不可见的扰动范围。
   - **蓝色点**：通过梯度下降更新后的图像。
   - **橙色点**：由于蓝色点跑出了 `ε` 的范围，我们需要把它“投影”到最近的边界上，让它回到可接受的扰动范围内。

   总而言之，这张幻灯片的核心思想是：**对抗攻击将传统的模型训练过程反转，通过梯度下降的方式，不是去优化模型的参数，而是去寻找一个能够欺骗模型、且扰动极其微小的输入图像。**

   ![image-20250910165326045](./pic/image-20250910165326045.png)

   ### 白箱与黑箱

   我们知道我们要攻击的模型的参数，这种叫**白箱攻击（White Box Attack）**。一般线上的模型不会告诉你参数。

   不知道模型参数的攻击叫黑箱攻击。

   **代理模型攻击 (Proxy Attack)**指的是可以自己本地整一个已经训练好的网络并攻击。然后再拿对抗图像去线上测试。

   要是连训练资料都没有，那就把很多图片丢给线上的模型然后看它的输出。再把输入输出的材料拿去训练模型，有可能训练一个类似的模型。

   ![image-20250910171011485](./pic/image-20250910171011485.png)

   黑箱攻击很容易成功。但也仅限于非定向攻击，定向攻击也不容易成功。

   如下图第一个图表所示，对角线是白箱攻击，其余是黑箱攻击。

   第二个图表是**集成攻击 (Ensemble Attack)**，列是被攻击网络，行是除了写上了网络的代理网络。即对角线是黑箱攻击，其余是白箱攻击。

   ![image-20250910171901253](./pic/image-20250910171901253.png)

   

有些人认为，模型容易受到攻击是因为数据集而非模型的问题。因为一张图片在各个模型下的特征类似。攻击只需要找到对应特征就好。

![image-20250910172445800](./pic/image-20250910172445800.png)

单像素攻击

单像素攻击并不强力。

![image-20250910172542704](./pic/image-20250910172542704.png)

**普遍对抗性攻击 universal adversarial attack** 

对每一个图像都要训练出对抗图片成本太高。也有人找到一个图片叠加在不同的图像上就会让模型失效。

![image-20250910172922801](./pic/image-20250910172922801.png)

除了图片，语音也有这样的问题。例如用机器识别合成的声音，人耳和机器可以识别出来。但是加入噪音之后，人耳听得没啥区别，但是机器就识别不出来这是合成的了。

自然语言处理模型也可以被攻击，加上一段文字之后，模型总会输出这一段文字了。

![image-20250910173234813](./pic/image-20250910173234813.png)

**对抗重编程**（Adversarial Reprogramming）是一种非常特殊的对抗攻击方式，它不只是为了让模型做出错误的分类，而是通过添加一种特殊的**对抗性输入**，来“劫持”一个预训练好的模型，让它执行一个**全新的任务**，并且这个过程不需要改变模型本身的任何参数。

例如我要训练一个模型，用来计算图片中方块的数量。有一个图像辨识网络，我可以攻击这个模型，让它看到特定几个方块时输出特定的东西。

![image-20250910174621055](./pic/image-20250910174621055.png)

在训练模型时就开后门。例如在训练集中加入一个对抗图片（狗），训练好后给模型输入鱼却被辨认为狗。

测试其他图片都没问题，唯独输入鱼的时候才会发现模型被攻击了。

这就需要注意网络上的训练集。

![image-20250910174942814](./pic/image-20250910174942814.png)

### 防御 Defense

**被动防御 Passive Defense**

例如在测试集输入模型之前，用一个过滤器把噪音过滤掉。例如把图片稍微的模糊一下。

![image-20250910175206727](./pic/image-20250910175206727.png)

当然模糊也会有其他影响，比如正确率降低。

![image-20250910175312981](./pic/image-20250910175312981.png)

被动防御也可以做压缩再解压缩，或者让生成器重新生成一样的图片。

![image-20250910175446589](./pic/image-20250910175446589.png)

但是如果在生成对抗图片时就已经采取了相应的措施（例如模糊化），那可能被动防御的效果就大打折扣。

防御方也可以用随机化处理图片的方式解决这类问题。避免了攻击方提前知道被动防御的具体处理方式。当然攻击方若是知道防御方随机的分布，或者用普遍攻击方式，也可能会攻破防御。

![image-20250910180016967](./pic/image-20250910180016967.png)

**主动防御 Proactive Defense**

**对抗训练 Adversarial Training** :训练一个很难被攻击的模型

给一个训练集和，每一项是图片和正确的标签。训练完成之后，把这个集和中的每一个图片都变成被对抗图片，但是标签仍然是正确的。这种改变数据集方法也可以算作**数据增强 Data Augmentation**。

这种训练也可以多持续几轮。

这种训练方式无法防御新算法的攻击，需要的计算资源也很大。

![image-20250910180744540](./pic/image-20250910180744540.png)

## 机器学习的可解释性 Explainable ML

机器能得到正确的答案并不意味着它很聪明。

例如，银行需要用机器判断是否要给客户发放贷款，但是必须要解释为什么要发放。**即机器学习不止要得到答案，还需要得到答案的理由。**

深度学习网络往往被看成一个黑盒子，可解释性比较差。

可解释性其实没有一个统一的标准，笼统的讲机器需要给人做出一个可以接受的理由。

可解释性被分为两部分：

* local Explanation：例如根据某一张猫图片判断这是一只猫，机器会告诉你为什么它认为这是一只猫。
* global Explanation：不需要给机器输入一张猫的图片，对一个模型中的参数而言，一只猫应该长什么样子。

### local Explanation

哪一部分对于机器的判别是至关重要的？

输入一张猫图，里边的每一个像素都是一个token，这些token的集合输入给机器，当改变某一个token时，输出参数发生了巨大的变化，那么可以说这个token就是重要的component。

![image-20250915101002691](./pic/image-20250915101002691.png)

我们可以把输入的token其中某一个加上一个Δ，这样计算出的loss也会有变化。

Δe比上Δx就是loss对这个token的偏微分，值越大则这个token对于图片的判断越重要。

显示图片中像素重要性的**热力图**叫**Saliency Map**。

![image-20250915101513166](./pic/image-20250915101513166.png)

如何让Saliency Map画的更好：**SmoothGrad**.

给原始图片加上不同的噪音变成N张图片，然后让机器画Saliency Map。最终把这些图片结合起来就能得到更好的SaliencyMap.

![image-20250915102606074](./pic/image-20250915102606074.png)

神经网络是如何处理输入数据呢？人类想知道这个过程。

例如声音识别，每一层神经网络都有100个单元，那么输出就是100维向量。我们可以用各种方法把这100维向量降维成2维向量，然后就可以用可视化的方法表示出来了。

![image-20250915103419293](./pic/image-20250915103419293.png)

如下图所示，经过八层的神经网络的输出以及能够根据讲话的内容把不同人声音的片段凑到一起。

![image-20250915103730183](./pic/image-20250915103730183.png)

 在BERT中，需要看某一层输出的embedding是什么。我们可以训练几个分类器，判断某个embedding是否为名词或动词，人名或地点等。这些用训练的分配器判断网络层中间输出的向量叫**探针**。

![image-20250915104758846](./pic/image-20250915104758846.png)

探针也可以不是分类器。例如语音识别网络中的某一层扔给了TTS，TTS再输出和输入语音相同的语音。但是还原的语音听不出来是谁讲的，这就说明这个语音识别网络会抹去说话的人的特征。

![image-20250915105201502](./pic/image-20250915105201502.png)

### global explanation

CNN中，输入一张图片，经过一些卷积层，每一个卷积层都有很多卷积核，每一个卷积核输出的激活值都生成一层特征图。

例如下图中，卷积核1在输入图片不同的位置侦测到了很多特征，那么输出特征值就会变大。

但是我们目前没有图片，需要创造能让这个卷积核输出特征比较大的图片。

![image-20250915111401908](./pic/image-20250915111401908.png)

我们需要找到一张图片，让卷积核1输出的激活值总和越大越好。

![image-20250915111531175](./pic/image-20250915111531175.png)

在CNN中数字像什么？

以下图为例，机器觉得长得最像数字0~9的图片如下所示。类似对抗图片，只要机器能识别到关键特征就能以很高的准确度输出结果。

![image-20250915111957961](./pic/image-20250915111957961.png)

如何让机器生成的最优图片转化为人眼能识别的图片？我们需要给最优化公式加上额外的函数。

例如R(x)就是笔画，函数的意思是笔画越少越好。

![image-20250915112424618](./pic/image-20250915112424618.png)

我们可以用生成器限制图片分类器得出的最优图片。

我们需要先训练好一个生成器，输入一个低维向量，输出一个图片。

在测试中，我们把低维向量z输入到生成器，然后把图片生成器输出的图片再给分类器，由于生成器输出的图片是人眼可识别的，我们就需要把这些图片作为分类器给出最高分数的图片。

![image-20250915113637415](./pic/image-20250915113637415.png)

这页PPT讲述的LIME方法，是通过在待解释样本的局部范围内，用一个简单的、可解释的模型（如线性模型）去拟合一个复杂的、不可解释的黑盒模型（如神经网络）的预测行为。然后，通过分析这个简单模型的决策依据，来为黑盒模型的单次预测提供一个直观、易懂的解释。

![image-20250915113854918](./pic/image-20250915113854918.png)

## 领域自适应 Domain Adaptation

数字识别很简单。但是如果训练集用的是黑白，测试集用的是彩色，那正确率就会大打折扣。

**Domain Shift**：训练和测试资料有不同的分布。

![image-20250915163546050](./pic/image-20250915163546050.png)

以文字识别为例，若有颜色的测试集只是很少但被打上标签。其实可以微调训练好的模型。

注意不要过拟合。

![image-20250915163939251](./pic/image-20250915163939251.png)

若有大量的彩色文字数据集但是没有打标签，那么我们就需要用一个特征提取器抽取黑白和彩色数字的共同特征，让机器学习忽略颜色判断数字。

![image-20250915164322611](./pic/image-20250915164322611.png)

但是既然测试集是没有打标签的，特征提取器是怎么知道测试集对应的数字？

**它不知道。**

但是，我们可以利用它来训练**特征提取器**，让它学会生成**领域不变的特征 (Domain-Invariant Features)**。

最常见的方法是引入一个**领域判别器 (Domain Discriminator)**，这是一种对抗训练（Adversarial Training）的思路：

1. **特征提取器 (Feature Extractor)**：它的**新目标**不仅是提取分类特征，还要努力**欺骗**领域判别器。它会尝试处理输入的黑白图片和彩色图片，生成一些特征（图中的橙色条），并让这些特征看起来尽可能的相似，使得判别器无法区分这些特征是来自黑白图片还是彩色图片。
2. **领域判别器 (Domain Discriminator)**：它的目标则相反，它要努力**分辨**出特征提取器生成的特征向量究竟是来自源域（黑白）还是目标域（彩色）。

**训练过程就像一个博弈：**

- 从源域（黑白）和目标域（彩色）各取一批图片。
- 特征提取器将它们都转换成特征向量。
- 领域判别器来判断每个特征向量的来源。
- 如果判别器**轻松地**分辨出来了（例如，它说“这个特征来自彩色图片”，并且说对了），那么就给特征提取器一个“惩罚”（通过反向传播更新其权重），告诉它：“你生成的特征差异太大了，再去调整一下，让它们更相似！”
- 如果判别器**被骗了**，无法分辨特征的来源，那就说明特征提取器的工作做得很好，它成功地抹去了特征中的领域信息（颜色、背景等）。

特征提取器只是通过与领域判别器的“对抗”，被迫学会了一种能力：无论给它一张黑白的“4”还是一张彩色的“4”，它都能提取出非常相似的、只包含“4”的形状信息的特征向量。

如下图所示，这是一整个分类器。特征提取器需要让后边的标签预测器判断不出来这个向量是由彩色图片还是由黑白图片生成的。

![image-20250915165549130](./pic/image-20250915165549130.png)

补充：既然鉴别器需要判断特征提取器生成的向量是否为黑白或彩色，那么这个算是一种标签吗？这种训练方式算是监督学习吗？

1. 鉴别器用的“黑白/彩色”信息算是标签吗？

**是的，绝对算！**

这个标签我们称之为**“领域标签” (Domain Label)**。

- 对于从源域（Source）来的黑白数字图片，我们给它打上领域标签，比如 “0”。
- 对于从目标域（Target）来的彩色数字图片，我们给它打上领域标签，比如 “1”。

这个标签的生成是**完全自动的**，我们不需要人工去标注。因为在准备数据的时候，我们本来就知道哪些数据来自源数据集，哪些来自目标数据集。

2. 这种训练方式算是监督学习吗？

这个问题需要分两个层面来看，这也是最容易混淆的地方：

**层面一：对于领域鉴别器 (Domain Discriminator) 自身而言，是监督学习。**

鉴别器的任务非常单纯：输入一个特征向量，输出它来自领域“0”还是领域“1”的概率。因为它有非常明确的输入（特征向量）和与之对应的、已知的正确答案（领域标签0或1），所以**训练鉴别器本身是一个彻头彻尾的监督学习（Supervised Learning）任务**。

**层面二：对于整个领域自适应 (Domain Adaptation) 任务而言，是非监督的。**

我们整个任务的**最终目的**，是让模型能够**正确地给无标签的彩色数字分类**（比如判断出某个彩色图片是数字“4”）。

在这个最终目的上，我们**没有任何彩色数字的类别标签**（即“4”、“6”这些标签）。因为目标域的类别标签是未知的，所以从这个角度看，整个任务属于**无监督领域自adaptive (Unsupervised Domain Adaptation)**。



具体来说，特征提取器输出的向量要让标签预测器的loss最小，同时要让领域分类器的loss最大（因为生成器要骗过鉴别器）。所以特征提取器的最优参数为：
$$
\theta^*_f =\min_{\theta_f}L + \max_{\theta_f}L_d \\
=\min_{\theta_f}L + \min_{\theta_f}(-L_d) \\
= \min_{\theta_f}(L - L_d)
$$
![image-20250915170036087](./pic/image-20250915170036087.png)

领域自适应方法的关键局限性

简单来说，即使我们成功地将源域（source）和目标域（target）的特征分布对齐了（如左图所示），模型学习到的决策边界（分类线）也**不一定对目标域的数据是最优的**，这可能导致分类错误。

假设你要学习区分“苹果”和“梨”。

- **源域 (Source Domain):** 你有大量的、带标签的“红富士苹果”和“绿鸭梨”的照片。你学习到了一条决策边界：“红色的就是苹果，绿色的就是梨”。
- **目标域 (Target Domain):** 现在给你一堆没有标签的“青苹果”和“黄香梨”的照片。
- **对抗训练 (Adversarial Training):** 你通过训练，让模型学会忽略颜色（领域特征），只关注形状。现在，模型不再依赖颜色来判断，特征分布被“对齐”了。
- **局限性出现:** 模型可能仍然沿用它从源域学到的基于形状的微小差异来划分。但万一“青苹果”的形状恰好稍微有点像“绿鸭梨”怎么办？模型可能会把很多“青苹果”**错误地、但却非常自信地**分类为“梨”，因为它只知道源域中的那两种水果的形状标准。

![image-20250915172316720](./pic/image-20250915172316720.png)

## 强化学习 Reinforcement Learning

当任务很困难，人类也不知道答案是什么的时候，可以考虑用RL

举例：玩space invader。有外星人，盾牌和母舰。当外星人全部被消灭时则任务成功。

![image-20250916103143617](./pic/image-20250916103143617.png)

当输出让母舰向左向右移动的指令时由于不会消灭任何外星人，故reward为0.

当母舰开火时，可能会消灭外星人，假设reward为5.

我们的目标是找到一个actor能让得分最大。

![image-20250916103256259](./pic/image-20250916103256259.png)

例如下围棋，实际上大部分时间的reward都是零。

![image-20250916103625203](./pic/image-20250916103625203.png)

### RL的步骤

和ML类似，都是三个步骤。

第一步：创建方程，根据当前游戏画面输出不同操作的不同的分数。和分类类似。

![image-20250916104056054](./pic/image-20250916104056054.png)

第二步：定义loss

![image-20250916104652331](./pic/image-20250916104652331.png)

上面图片的互动会一直持续下去直到游戏结束。把一整轮游戏叫一个episode。

![image-20250916104824659](./pic/image-20250916104824659.png)

第三步：最优化

找到一组网络的参数，让R的数值越大越好。其中整个交互序列叫**轨迹 Trajectory, τ**。

但是这个巨大的网络是有随机性的。例如在第一个actor中，即使输入的sample一样输出的a1也不一定一样。而且Env（这里是游戏机）和reward是一个黑盒子，我们不知道发生了什么，同时env和reward也是有随机性的。

所以RL真正的难点是如何解决最优化问题。

RL和GAN很类似。

| 特征         | 强化学习 (RL)                                | 生成对抗网络 (GAN)                                           |
| ------------ | -------------------------------------------- | ------------------------------------------------------------ |
| **生成模块** | **Actor (策略网络)**                         | **Generator (生成器)**                                       |
| **生成物**   | **Action (动作序列)**                        | **Fake Data (伪造数据, 如图片)**                             |
| **评估模块** | **Environment + Reward (环境+奖励)**         | **Discriminator (判别器)**                                   |
| **评估信号** | **Total Reward (总回报 `R`)** (一个标量分数) | **Realism Score (真实度得分)** (一个标量概率)                |
| **优化目标** | 调整 **Actor**，最大化 **Reward**            | 调整 **Generator**，最大化 **Discriminator** 犯错的概率（即让它认为假的是真的） |

**RL和GAN的关键区别 (The Core Challenge Highlighted by the Analogy):**

这正是这个类比最精妙的地方，它突显了RL的独特困境。

- 在 **GAN** 中，判别器（Discriminator）本身是一个**神经网络**。这意味着从生成器的输出（假图片）到判别器的最终得分，整个计算图是**完全可微分 (differentiable)** 的。因此，我们可以直接使用反向传播，将判别器给出的损失梯度一路传回给生成器，从而更新生成器。这个信息通路是通畅的。
- 在 **RL** 中，如PPT所强调，环境（Environment）是一个**黑箱 (black box)**，它是**不可微分的**。你无法对一个游戏引擎或者真实世界的物理规律求梯度。动作 `a` 输入到环境后，梯度信息就“断掉”了。

补充：RL的随机性拿ML类比是拿一个训练好的模型去测试，相同的input会输出不同的结果。

![image-20250916110459245](./pic/image-20250916110459245.png)

### 策略梯度 Policy Gradient

如何控制actor？类似分类问题，我们需要给适当的标签，让actor做特定行为或者不做特定行为（让loss最小或最大）。

![image-20250916115024377](./pic/image-20250916115024377.png)

当sample不一样时，我们可以让actor输出不同的结果。例如看到上面向左，看到下面**不要向右**。

这个loss公式的完整意思是：“**请调整Actor，让它在 `s` 状态下‘向左’的倾向，要远大于它在 `s'` 状态下‘向右’的倾向。**” 这是一种相对的、比较式的学习。

![image-20250916120543524](./pic/image-20250916120543524.png)

把每一个标签和对应的行为都打上分数，不一定非得是二元分类。

难点是如何找到标签和行为的匹配及其分数。

这样的loss就是：
$$
L = \sum A_n e_n
$$
![image-20250916121648013](./pic/image-20250916121648013.png)

可以先让一个随机的actor，输入s输出a。多跑几轮就会有一堆训练集。可以令A是reward。但这样操作会让整个网络变得短视。

![image-20250916122638354](./pic/image-20250916122638354.png)

显而易见，每一个行为及输出都不是独立的。

同时也有一个概念：**Reward delay**，可能需要牺牲短期利益换取长期目标。

例如这个游戏，网络只会学习到一直开火就好了。

![image-20250916122956544](./pic/image-20250916122956544.png)

改进版：例如执行出来a1之后，把reward从头加到尾作为loss的权重A；同理a2。

这个G叫做**cumulated reward**。
$$
G_t = \sum^N_{n=t}r_n
$$
![image-20250916124242925](./pic/image-20250916124242925.png)

继续优化

如果游戏流程特别长，那么$r_N$可能跟一开始的步骤$r_1$关系不大。我们可以用下面的公式计算权重：
$$
G_t ' = \sum^N_{n=t}\gamma^{n-t} r_n, \ \ 
0<\gamma<1
$$
![image-20250916124731471](./pic/image-20250916124731471.png)

继续优化

我们需要把reward标准化。如果reward最低分就是10分，某一步骤拿到了10分人感觉分数很差，但是机器觉得还可以。

可以设定一个baseline，G减去b得到A，然后让A有正有负。

![image-20250916125408803](./pic/image-20250916125408803.png)

策略梯度算法的**核心训练流程**。

这个算法的数据收集是在训练的循环之内的，这是这类算法的核心特点：**On-Policy (同策略)**。

意思就是，用来训练第 `i` 代Actor的数据，必须是由第 `i-1` 代Actor**亲自**去玩、去收集的。你不能用一个很旧的、很笨的Actor（比如第1代）收集的数据来训练一个比较新的、更聪明的Actor（比如第10代）。

这种训练模式和普通的监督学习有什么区别？

| 特征对比                | 普通监督学习 (Supervised ML)                                 | 策略梯度 (Policy Gradient, RL)                               |
| ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **1. 数据的来源**       | **静态的、预先收集好的数据集**。数据在训练开始前就已确定，并且在整个训练过程中**保持不变**。 | **动态的、由模型自己生成的数据**。数据是在**每一个训练循环内部**由当前的模型（Actor）与环境互动而产生的。 |
| **2. “正确答案”的定义** | **明确的标签 (Label)**。对于每个输入 `X`，都有一个唯一的、正确的输出 `y`。学习目标是模仿这些标签。 | **没有明确的“正确动作”**。只有一个**评估性的分数 (Reward/Advantage)**，告诉你某个动作是好是坏，但不会告诉你“正确的动作”是什么。 |
| **3. 反馈信号**         | **指导性的、即时的**。模型做出预测后，可以立刻通过与正确标签对比，得到明确的误差，指导模型如何修正。 | **评估性的、延迟的**。模型可能需要执行一长串动作后才能得到一个最终的奖励（比如下完一盘棋才知道输赢）。如何把这个最终结果分配给过程中的每一步（信用分配问题），是RL的核心难点。 |
| **4. 数据的使用方式**   | **可重复使用**。同一个数据集可以被反复用来训练模型（称为训练多个**Epoch**），以充分榨取数据中的信息。 | **一次性的 (On-Policy)**。如PPT注释所说，数据由当前策略生成。参数更新后，旧策略产生的数据就“过时”了，**必须被丢弃**，然后用新策略去收集新数据 |

**用一个生动的比喻来理解**

- **普通监督学习就像是“学生备考”**：
  - **数据**：一本习题册，上面有题目（输入 `X`）和标准答案（标签 `y`）。这本习题册是固定的。
  - **训练**：学生反复做这本习题册上的题（训练多个Epoch）。每做一道题，就立刻对一下答案，知道自己哪里错了（计算Loss）。
  - **目标**：通过反复练习，学会所有题目的解法，最终能在考试中举一反三。
- **策略梯度 (RL) 就像是“学习滑雪”**：
  - **数据**：没有现成的“滑雪教科书”。你必须亲自穿上雪板，从山坡上滑下去（与环境互动），这个过程就是**数据的生成**。你这次滑行的轨迹、摔了多少次跤，就是你这一轮的训练数据。
  - **训练**：滑完一次后，你的教练（算法）会给你反馈：“你刚才那个转弯不错（正向Advantage），但是重心太靠后了导致摔倒（负向Advantage）。”（计算`A`和`L`）。你根据这个反馈，调整自己的姿势和技巧（更新参数 `θ`）。
  - **关键区别**：你用**调整后的新姿势**去滑下一次时，你的滑行轨迹（**新数据**）肯定和上一次不一样了。你不能通过反复观看你第一次滑雪摔得一塌糊涂的录像（**旧数据**）来提高你现在的水平。**你必须用你当前的能力去实践，才能获得对当前最有用的反馈。** 这就是“数据收集在循环内”的本质。

![image-20250916131647600](./pic/image-20250916131647600.png)

所以每训练好一次需要重新收集一次资料。这个过程非常的花时间

![image-20250916131710675](./pic/image-20250916131710675.png)

On-policy 与 off-policy

Off-policy的好处是可以拿着旧资料训练新的actor。收集一次资料就可以训练很多轮。

![image-20250916132503830](./pic/image-20250916132503830.png)

在训练过程中，拿去和环境互动的actor的随机性非常重要，这样会收集到丰富的资料。例如如果只知道移动不知道开火那么reward永远都是0.

我们会刻意的给输出加上一些噪音等，加大交叉熵。

![image-20250916132926084](./pic/image-20250916132926084.png)

### Actor-Critic

Critic：判断一个actor θ 当观察到S时有多好

预测方程$V^θ(s)$：当用一个actor时，这个方程需要预测当actor输入s后带参数累加的reward。

预测方程不止跟输入画面s有关，也跟actor θ有关。一个相同的画面给不同的actor也会预测不一样的结果。

![image-20250916185945448](./pic/image-20250916185945448.png)

如何训练critic？有两种方法。

第一种是Monte-Carlo方法，如图所示

![image-20250916190345025](./pic/image-20250916190345025.png)

Temporal-difference approach

在上面的方法中，需要玩完整场游戏才能得到一个训练参数。而这个方法并不需要整场游戏就可以训练资料。

如PPT所示，相邻的两个游戏图片得出的V经过处理的得到的结果要和前一张图片reward做比较。

![image-20250916190946995](./pic/image-20250916190946995.png)

两种算法的比较，结果可能不相同。

因为MC方法是只看到给我们的资料，由于资料中只有一个$S_a$，故只会计算这个。

而TD假设了$S_a$的reward跟$S_b$没有关系，$S_a$输出reward不会影响到$S_b$。虽然仅有一个资料有$S_b$，但是客观上$S_b$是在$S_a$之后的，所以用TD公式计算结果如下。

![image-20250916191823771](./pic/image-20250916191823771.png)

cirtic如何用于训练actor上面？

在训练数据中，我们把评分A的计算公式后面的b改成了V。

![image-20250916193954831](./pic/image-20250916193954831.png)

为什么要这样算？因为G是一个actor的真实加权评分，再减去一个预估V。当A>0时表示当前actor的表现要好于平均值。
$$
\{S_t,\ a_t\} \ \ A_t = G'_t - V^\theta(s_t)
$$


注意，给一个actor输入一个$S_t$后，由于随机性的存在，可能会做出不同的action($a_t$)。这些action对应的G取一个特定的平均值能得到V。所以上面公式是在判断某一个actor做出action的reward加权（G）减去平均reward加权（V）得到的分数。



![image-20250916200659322](./pic/image-20250916200659322.png)

**Advantage Actor-Critic** 计算评分方法：

可以这么算，把相邻两个步骤平均下来的加权reward相减得到了平均的$r_t$，用真实的$r_t$减去这个$r_t$。
$$
\{S_t,\ a_t\} \ \ A_t = r_t + V^\theta(s_{t + 1}) - V^\theta(s_t)
$$
![image-20250916201322420](./pic/image-20250916201322420.png)

在训练actor-critic时，可以前面几层用相同的网络（例如CNN，识别游戏图像），后面再分化。

![image-20250916201542032](./pic/image-20250916201542032.png)

### 奖励塑造 Reward Shaping 

如果每一步actor执行的结果reward都差不多是零，那根本没办法去训练，这种状况叫**稀疏奖励 Sparse Reward**。

例如下围棋，整场游戏下完才有reward。例如机械臂打螺丝，只有把螺丝打进去才有reward。但是由于一开始的参数是随机的，机械臂没准只能随便在空中飞舞。

我们需要想办法额外提供reward引导**agent（学习者或者决策者）**学习。这种方式叫**Reward Shaping**。

![image-20250917103420016](./pic/image-20250917103420016.png)

例如一个在迷宫打怪的游戏，尽管只有杀死敌人加分，被敌人杀死扣分，但是可以认为的给一些行为加上reward。例如掉血扣分，损失弹药扣分等等，这些都不会影响游戏的实际分数。

![image-20250917104322515](./pic/image-20250917104322515.png)

### No Reward： Learning from Demonstration

假设今天连reward都没有呢？

人为给机器定制的规则与reward可能会导致意想不到的结果，例如机器人三定律，机器会得到“把人类囚禁起来”的最优解。

![image-20250917105116863](./pic/image-20250917105116863.png)

在没有reward时，我们会找很多的专家人类expert给agent做示范。例如自动驾驶，教机器打螺丝。

![image-20250917105331167](./pic/image-20250917105331167.png)

给机器输入匹配的图片和动作，这个也是一种监督学习，也叫做**Behavior Cloning**。

然而光让机器复制人类的行为也有很多问题。

问题1：所有人类专家只能给机器演示正确的操作。例如由于所有专家都会转弯，但是汽车在转弯快要出车祸时没有例子给机器示范。

![image-20250917105653439](./pic/image-20250917105653439.png)

问题2：有些人类的行为及其没必要模仿。

这样就引出了**逆向增强学习 Inverse reinforcement Learning**

agent根据环境和专家的动作逆推出reward公式。

![image-20250917110745497](./pic/image-20250917110745497.png)

 例如，reward function给老师比较高的分数，给actor比较低的分数，直到循环结束。

![image-20250917111241662](./pic/image-20250917111241662.png)

整个流程如下图所示。

![image-20250917111716397](./pic/image-20250917111716397.png)

IRL和GAN在流程上很类似。

![image-20250917111857262](./pic/image-20250917111857262.png)

 ## Life Long Learning 终身学习

 ![image-20250917125103414](./pic/image-20250917125103414.png)

   如下图所示，一个简单的网络学习好任务1之后，任务1和任务2的正确率都很好（尽管没学过任务２）

但是拿着这个训练好的模型再去做任务２的时候，任务１的正确率反而下降了。

![image-20250917125839330](./pic/image-20250917125839330.png)

但是对于这个网络同时学好任务１和任务２是办得到的。

![image-20250917130032738](./pic/image-20250917130032738.png)

再举一个例子，机器依次学习任务会忘记旧任务。一起学习反而正确率更好。

![image-20250917130431537](./pic/image-20250917130431537.png)

多任务训练尽管可以解决终身学习问题，但是对于一个网络要学习第1000个任务时要把前面所有的999个任务资料都要记住，这样开销还是太大了。

多任务训练确实要比终身学习训练要强，我们常常把多任务训练当成终身学习训练的上限。

![image-20250917132837063](./pic/image-20250917132837063.png)

Life Long和Transfer任务作比较：

Transfer是在一个任务训练好之后，在另一个任务做的也很好。

Lift long是在学习第二个任务时不要忘记第一个任务的训练成果。

![image-20250917133418691](./pic/image-20250917133418691.png)

 如何评估LL？

设一个$R_{i,j}$，依次训练好第i个任务时都要给所有的任务j做测试。表格中i>j的部分是看任务是否被遗忘，i<j部分是任务的transfer能力。

一般来说，最终的准确率是看表格最后一行的平均值（学完所有任务后再用所有任务做测试的平均分）。

还有另一个评估方法 **Backward Transfer**：当学完所有任务之后，依次拿着最后一行的任务分数减去对应这个任务刚刚训练好再测试的分数。这个值经常是负的，如果是正的说明模型训练的非常好。

 ![image-20250917134946065](./pic/image-20250917134946065.png)

Life Learning 的三个可能解法

**选择性突触可塑性 Selective Synaptic Plasticity**

我们需要先了解为什么**Catastrophic Forgetting 灾难性遗忘**为什么会发生。

在任务一用梯度下降训练好的参数拿到任务二中可能会找到任务2的最优解，但是这个参数拿回到任务1就不行了。

现在的优化方法是让参数移动到既能做好任务二又能做好任务1的位置。

![image-20250917144417381](./pic/image-20250917144417381.png)

有些参数对于旧任务很重要，在学习新任务时这些参数不要变。

 假设$θ^b$是从之前的任务学习到的参数，我们会给每一个参数$θ^b_i$一个守卫$b_i$

下面的公式中，L(θ)是当前任务的loss，优化目标是让其越小越好。后面叫做记忆惩罚项，假设我们不希望某一个$θ^b_i$被改变很多，那么前面乘的$b_i$就要越大越好，整个公式得到的loss就会随着当前的这个参数数值变化而产生非常明显地变化。同理，如果一个参数对于旧任务无关紧要，那么可以把$b_i$调小。

λ是一个超参数，用来判断新旧任务的重要性。要是觉得不能遗忘旧知识就把λ调大一点。
$$
L'(\theta) = L(\theta) + \lambda \sum_{i} b_i (\theta_i - \theta_i^b)^2
$$
![image-20250917150024561](./pic/image-20250917150024561.png)

 我们如何找到$b_i$？下面只是简单演示一下

![image-20250917150402866](./pic/image-20250917150402866.png)

如果我们把$b_i$都设定同一个值的话，会出现新任务学不起来的问题。如下图绿线所示。

给不同的参数不同的 $b_i$（红线），要比绿线好一点。

![image-20250917150717290](./pic/image-20250917150717290.png)

这是另一种求新参数的做法：Gradient Episodic Memory (GEM)

GEM不是在参数上做限制，而是在梯度下降的方向上做限制。在第二个任务找到梯度$g$后，需要知道上一个任务的梯度$g^b$，然后处理下得到新的梯度$g'$。这个新的梯度要求和g同向并且不能差太多。

![image-20250917152223557](./pic/image-20250917152223557.png)

curriculum learning ：有的模型先训练任务2再训练任务1要比正过来训练效果要好。研究任务训练顺序的问题叫~

## Network Compression 神经网络压缩

如果模型太巨那么模型在小设备上可能跑不动

### Network Pruning

把大的network中一些没有用的参数或神经元找出来然后剪掉。这样模型可能会掉一点准确度，但是可以重新微调一下。反复这个过程，直到得到一个小的network。

![image-20250917195915604](./pic/image-20250917195915604.png)

形状不一的network在写代码的时候不好写。也不能像下图中的右边，两个神经元用权值为零的参数连上，这样其实也是计算了的。

![image-20250917200231249](./pic/image-20250917200231249.png)

在实践中，剪掉了很多参数（如下图剪掉了95%），但是训练速度反而没变快。

![image-20250917200411569](./pic/image-20250917200411569.png)

在pytorch中，去掉一层中的神经元比较方便。

![image-20250917200747175](./pic/image-20250917200747175.png)

我们为什么不训练一个小的网络？反而是把大网络变小？因为大网络比小网络好训练。

为什么大网络比小网络好训练？根据大乐透假说：一个大网络只是很多小网络的组合，只要有一个小网络好用，整个网络运行结果就不错。

![image-20250917202321150](./pic/image-20250917202321150.png)

### Knowledge Distillation 知识蒸馏



